{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tiYugevDYQWk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TabularEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder for tabular data.\n",
        "    Maps input features to latent mean and log-variance for Gaussian latent space.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims, latent_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, h_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_dim = h_dim\n",
        "        self.feature_extractor = nn.Sequential(*layers)\n",
        "\n",
        "        self.mu_layer = nn.Linear(prev_dim, latent_dim)\n",
        "        self.logvar_layer = nn.Linear(prev_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, input_dim)\n",
        "\n",
        "        Returns:\n",
        "            mu: (batch_size, latent_dim)\n",
        "            logvar: (batch_size, latent_dim)\n",
        "        \"\"\"\n",
        "        h = self.feature_extractor(x)\n",
        "        mu = self.mu_layer(h)\n",
        "        logvar = self.logvar_layer(h)\n",
        "        return mu, logvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CS2EeGJwYYIS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Denoiser(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple denoiser model for tabular diffusion.\n",
        "    Applies a linear -> ReLU -> linear architecture.\n",
        "    Outputs numerical predictions and categorical probabilities (via softmax).\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_in, latent_dim, dim_hidden, num_numeric, categories):\n",
        "        super().__init__()\n",
        "        self.num_numeric = num_numeric\n",
        "        self.categories = categories\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim_in + latent_dim + 1, dim_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_hidden, dim_in)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, z, t):\n",
        "        \"\"\"\n",
        "        Forward pass of the denoiser.\n",
        "\n",
        "        Args:\n",
        "            x: Tensor, shape (batch_size, dim_in)\n",
        "            t: Tensor, shape (batch_size,)\n",
        "\n",
        "        Returns:\n",
        "            out_num: numerical denoised output\n",
        "            out_cat: categorical probabilities after softmax\n",
        "        \"\"\"\n",
        "        t = t.unsqueeze(1).float()\n",
        "        xzt = torch.cat([x, z, t], dim=1)\n",
        "        out = self.net(xzt)\n",
        "\n",
        "        out_num = out[:, :self.num_numeric]\n",
        "        out_cat_raw = out[:, self.num_numeric:]\n",
        "\n",
        "        out_cat = []\n",
        "        idx = 0\n",
        "        for K in self.categories:\n",
        "            logits = out_cat_raw[:, idx:idx+K]\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            out_cat.append(probs)\n",
        "            idx += K\n",
        "\n",
        "        out_cat = torch.cat(out_cat, dim=1) if out_cat else None\n",
        "        return out_num, out_cat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HwXSeIdKYYiv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import torch.nn.utils as nn_utils\n",
        "import itertools\n",
        "\n",
        "class ClusterDDPM:\n",
        "    \"\"\"\n",
        "    ClusterDDPM model: wraps encoder, denoiser, diffusion schedules, and GMM.\n",
        "    Supports pretraining, ELBO training, GMM fitting, and sampling.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, denoiser, T, num_numeric, categories, n_clusters, device):\n",
        "        self.encoder = encoder\n",
        "        self.denoiser = denoiser\n",
        "        self.T = T\n",
        "        self.num_numeric = num_numeric\n",
        "        self.categories = categories\n",
        "        self.n_clusters = n_clusters\n",
        "        self.device = device\n",
        "\n",
        "        # Diffusion schedules\n",
        "        betas = 0.01 * torch.arange(1, T + 1).float() / T\n",
        "        alphas = 1 - betas\n",
        "        self.alpha_bars = torch.cumprod(alphas, dim=0).to(device)\n",
        "        self.sqrtab = self.alpha_bars.sqrt()\n",
        "        self.sqrtmab = (1 - self.alpha_bars).sqrt()\n",
        "\n",
        "        self.gmm = None  # Will hold fitted GMM\n",
        "\n",
        "    def pretrain_step(self, x, optimizer):\n",
        "        \"\"\"\n",
        "        One pretraining step: predict noise from x_t + z + t\n",
        "        \"\"\"\n",
        "        B = x.shape[0]\n",
        "        t = torch.randint(1, self.T + 1, (B,), device=self.device) - 1\n",
        "        noise = torch.randn_like(x)\n",
        "\n",
        "        x_t = self.sqrtab[t].unsqueeze(1) * x + self.sqrtmab[t].unsqueeze(1) * noise\n",
        "\n",
        "        mu, logvar = self.encoder(x)\n",
        "        z = mu + torch.randn_like(mu) * (0.5 * logvar).exp()\n",
        "\n",
        "        t_norm = t.float() / self.T\n",
        "        pred_num, pred_cat = self.denoiser(x_t, z, t_norm)\n",
        "\n",
        "        if self.num_numeric > 0 and pred_num is not None:\n",
        "            noise_num = noise[:, :self.num_numeric]\n",
        "            loss_num = ((pred_num - noise_num) ** 2).mean()\n",
        "        else:\n",
        "            loss_num = torch.zeros((), device=x.device)\n",
        "\n",
        "        loss_cat = torch.zeros((), device=x.device)\n",
        "\n",
        "        has_cats = bool(self.categories) and sum(self.categories) > 0\n",
        "        if has_cats and pred_cat is not None:\n",
        "          x0_cat = x[:, self.num_numeric:]\n",
        "          idx_c = 0\n",
        "          for K in self.categories:\n",
        "              target = x0_cat[:, idx_c:idx_c+K]\n",
        "              pred_prob = pred_cat[:, idx_c:idx_c+K]\n",
        "              kl = (target * (torch.log(target + 1e-10) - torch.log(pred_prob + 1e-10))).sum(1).mean()\n",
        "              loss_cat += kl\n",
        "              idx_c += K\n",
        "          if len(self.categories) > 0:\n",
        "              loss_cat /= len(self.categories)\n",
        "\n",
        "        loss = loss_num + loss_cat\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn_utils.clip_grad_norm_(\n",
        "            itertools.chain(self.encoder.parameters(), self.denoiser.parameters()),\n",
        "            max_norm=1.0\n",
        "        )\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss.item(), loss_num.item(), loss_cat.item()\n",
        "\n",
        "\n",
        "    def pretrain(self, dataloader, optimizer, epochs, batch_size, plot_freq=100):\n",
        "        \"\"\"\n",
        "        Pretrain encoder + denoiser over multiple steps.\n",
        "\n",
        "        Args:\n",
        "            dataloader: full dataset tensor (N, D)\n",
        "            optimizer: optimizer for encoder + denoiser\n",
        "            epochs: number of pretraining epochs\n",
        "            batch_size: batch size\n",
        "            plot_freq: print loss every plot_freq steps\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            n_samples = 0\n",
        "            for (x_batch,) in dataloader:\n",
        "                if x_batch.ndim == 1:\n",
        "                    x_batch = x_batch.unsqueeze(0)\n",
        "                x_batch = x_batch.to(self.device)\n",
        "\n",
        "                loss, loss_num, loss_cat = self.pretrain_step(x_batch, optimizer)\n",
        "\n",
        "                total_loss += loss * x_batch.size(0)\n",
        "                n_samples += x_batch.size(0)\n",
        "\n",
        "            avg_loss = total_loss / n_samples\n",
        "            if (epoch+1) % plot_freq == 0:\n",
        "              print(f'[Pretrain] Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "\n",
        "    def fit_gmm(self, dataloader):\n",
        "        \"\"\"\n",
        "        Fit a Gaussian Mixture Model on the latent space.\n",
        "\n",
        "        Args:\n",
        "            x_real: full dataset tensor (N, D)\n",
        "            n_clusters: number of clusters to fit\n",
        "        \"\"\"\n",
        "        self.encoder.eval()\n",
        "        latent_z = []\n",
        "        with torch.no_grad():\n",
        "            for (x,) in dataloader:\n",
        "                x = x.to(self.device)\n",
        "                z_mu, z_sigma2_log = self.encoder(x)\n",
        "                z = torch.randn_like(z_mu) * torch.exp(z_sigma2_log / 2) + z_mu\n",
        "                latent_z.append(z)\n",
        "        latent_z = torch.cat(latent_z, 0).detach().cpu().numpy()\n",
        "\n",
        "        if self.gmm is not None:\n",
        "            init_weights = self.gmm.weights_\n",
        "            init_means = self.gmm.means_\n",
        "            init_precisions = self.gmm.precisions_\n",
        "            gmm = GaussianMixture(n_components = self.n_clusters,\n",
        "                                  covariance_type = 'diag',\n",
        "                                  reg_covar=1e-2,\n",
        "                                  weights_init = init_weights,\n",
        "                                  means_init = init_means,\n",
        "                                  precisions_init = init_precisions)\n",
        "        else:\n",
        "          gmm = GaussianMixture(n_components=self.n_clusters, covariance_type='diag', reg_covar=1e-2)\n",
        "        gmm.fit(latent_z)\n",
        "        self.gmm = gmm\n",
        "        #print(f\"✅ GMM fitted with {self.n_clusters} components\")\n",
        "\n",
        "\n",
        "    def elbo_step(self, x, optimizer, kl_weight=0.1):\n",
        "        \"\"\"\n",
        "        One ELBO training step for ClusterDDPM.\n",
        "\n",
        "        Args:\n",
        "            x: input batch (B, D)\n",
        "            optimizer: optimizer\n",
        "            kl_weight: weight for the KL terms\n",
        "\n",
        "        Returns:\n",
        "            total_loss, rec_loss, kl_loss\n",
        "        \"\"\"\n",
        "        B = x.shape[0]\n",
        "        t = torch.randint(1, self.T + 1, (B,), device=self.device) - 1\n",
        "        noise = torch.randn_like(x)\n",
        "\n",
        "        # Diffusion forward process\n",
        "        x_t = self.sqrtab[t].unsqueeze(1) * x + self.sqrtmab[t].unsqueeze(1) * noise\n",
        "\n",
        "        # Encoder\n",
        "        mu_phi, logvar_phi = self.encoder(x)\n",
        "        sigma2_phi = torch.exp(logvar_phi)\n",
        "        z = mu_phi + torch.randn_like(mu_phi) * (0.5 * logvar_phi).exp()\n",
        "\n",
        "        # Denoising\n",
        "        t_norm = t.float() / self.T\n",
        "        pred_num, pred_cat = self.denoiser(x_t, z, t_norm)\n",
        "\n",
        "        # Reconstruction loss\n",
        "        if self.num_numeric > 0 and pred_num is not None:\n",
        "            noise_num = noise[:, :self.num_numeric]\n",
        "            rec_loss_num = ((pred_num - noise_num) ** 2).mean()\n",
        "        else:\n",
        "            rec_loss_num = torch.zeros((), device=x.device)\n",
        "\n",
        "        rec_loss_cat = 0.0\n",
        "        x0_cat = x[:, self.num_numeric:]\n",
        "        idx_c = 0\n",
        "        for K in self.categories:\n",
        "            target = x0_cat[:, idx_c:idx_c + K]\n",
        "            pred_prob = pred_cat[:, idx_c:idx_c + K]\n",
        "            kl = (target * (torch.log(target + 1e-10) - torch.log(pred_prob + 1e-10))).sum(1).mean()\n",
        "            rec_loss_cat += kl\n",
        "            idx_c += K\n",
        "        if len(self.categories) > 0:\n",
        "            rec_loss_cat /= len(self.categories)\n",
        "\n",
        "        rec_loss = rec_loss_num + rec_loss_cat\n",
        "\n",
        "        # GMM prior\n",
        "        pi = torch.tensor(self.gmm.weights_, device=self.device, dtype=torch.float32)\n",
        "        c_mu = torch.from_numpy(self.gmm.means_).to(x).float()\n",
        "        c_var = torch.from_numpy(self.gmm.covariances_).to(x).float()\n",
        "        c_logvar = torch.log(c_var)\n",
        "\n",
        "        # p(c|z) = soft responsibilities\n",
        "        det = 1e-10\n",
        "        log_prob_c = []\n",
        "        for c in range(self.gmm.n_components):\n",
        "            logp = -0.5 * (\n",
        "                torch.sum(np.log(2 * np.pi) + c_logvar[c] +\n",
        "                        (z - c_mu[c]) ** 2 / c_var[c], dim=1)\n",
        "            )\n",
        "            log_prob_c.append((torch.log(pi[c] + det) + logp).unsqueeze(1))\n",
        "        log_prob_c = torch.cat(log_prob_c, dim=1)\n",
        "        logsumexp = torch.logsumexp(log_prob_c, dim=1)\n",
        "        w_c = torch.exp(log_prob_c - logsumexp.unsqueeze(1))  # (B, K)\n",
        "\n",
        "        # Cat KL: -λ sum_c w_c log (pi / w_c)\n",
        "        cat_kl = - (w_c * (torch.log(pi + det) - torch.log(w_c + det))).sum(1).mean()\n",
        "\n",
        "        # Gaussian KL\n",
        "        gauss_kl = 0.5 * (\n",
        "            w_c[:, :, None].squeeze(2) * (\n",
        "                c_logvar[None, :, :] +\n",
        "                sigma2_phi[:, None, :] / c_var[None, :, :] +\n",
        "                (mu_phi[:, None, :] - c_mu[None, :, :]) ** 2 / c_var[None, :, :]\n",
        "            ).sum(2)\n",
        "        ).sum(1).mean()\n",
        "\n",
        "        # Variational entropy term\n",
        "        entropy = -0.5 * (1 + logvar_phi).sum(1).mean()\n",
        "\n",
        "        kl_loss = cat_kl + gauss_kl + entropy\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = rec_loss + kl_weight * kl_loss\n",
        "\n",
        "        # Backward + update\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        nn_utils.clip_grad_norm_(\n",
        "            itertools.chain(self.encoder.parameters(), self.denoiser.parameters()),\n",
        "            max_norm=1.0\n",
        "        )\n",
        "        optimizer.step()\n",
        "\n",
        "        return total_loss.item(), rec_loss.item(), kl_loss.item()\n",
        "\n",
        "\n",
        "\n",
        "    def train_elbo(self, dataloader, optimizer, batch_size, kl_weight=0.1, plot_freq=100):\n",
        "        \"\"\"\n",
        "        ELBO training loop: combines reconstruction + KL loss.\n",
        "\n",
        "        Args:\n",
        "            x_real: dataset (N, D)\n",
        "            optimizer: optimizer\n",
        "            batch_size: batch size\n",
        "            kl_weight: weight on KL\n",
        "            plot_freq: print every plot_freq steps\n",
        "        \"\"\"\n",
        "        self.encoder.train()\n",
        "        self.denoiser.train()\n",
        "        total_loss = 0.0\n",
        "        total_recon_loss = 0.0\n",
        "        total_kl_loss = 0.0\n",
        "        n_samples = 0\n",
        "\n",
        "        for (x_batch,) in dataloader:\n",
        "            if x_batch.ndim == 1:\n",
        "                x_batch = x_batch.unsqueeze(0)\n",
        "            x_batch = x_batch.to(self.device)\n",
        "            loss, rec_loss, kl_loss = self.elbo_step(x_batch, optimizer, kl_weight=kl_weight)\n",
        "\n",
        "            total_loss += loss * x_batch.size(0)\n",
        "            total_recon_loss += rec_loss * x_batch.size(0)\n",
        "            total_kl_loss += kl_loss * x_batch.size(0)\n",
        "            n_samples += x_batch.size(0)\n",
        "\n",
        "        avg_loss = total_loss / n_samples\n",
        "        avg_recon_loss = total_recon_loss / n_samples\n",
        "        avg_kl_loss = total_kl_loss / n_samples\n",
        "        return avg_loss, avg_recon_loss, avg_kl_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB3jQxzWYYww",
        "outputId": "dd107394-9139-4184-92c7-efc51b7d08d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dim_hidden 500, latent_dim: 5\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.7744\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.7379\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.7334\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.7045\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.7000\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.6761\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.6726\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.6643\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.6500\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.6506\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "Epoch 100/1000, Loss: 0.8442, Recon-Loss: 0.8408, KL-Loss: 0.0336\n",
            "Epoch 200/1000, Loss: 0.8333, Recon-Loss: 0.8317, KL-Loss: 0.0160\n",
            "Epoch 300/1000, Loss: 0.8011, Recon-Loss: 0.7985, KL-Loss: 0.0268\n",
            "Epoch 400/1000, Loss: 0.8143, Recon-Loss: 0.8135, KL-Loss: 0.0085\n",
            "Epoch 500/1000, Loss: 0.7813, Recon-Loss: 0.7808, KL-Loss: 0.0043\n",
            "Epoch 600/1000, Loss: 0.7815, Recon-Loss: 0.7812, KL-Loss: 0.0025\n",
            "Epoch 700/1000, Loss: 0.7613, Recon-Loss: 0.7608, KL-Loss: 0.0052\n",
            "Epoch 800/1000, Loss: 0.7804, Recon-Loss: 0.7797, KL-Loss: 0.0075\n",
            "Epoch 900/1000, Loss: 0.7478, Recon-Loss: 0.7473, KL-Loss: 0.0045\n",
            "Epoch 1000/1000, Loss: 0.7427, Recon-Loss: 0.7417, KL-Loss: 0.0098\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.9148\n",
            "ARI: 0.0000\n",
            "Saved results to clutad0/clusterddpm_results.json\n",
            "\n",
            "💾 New best model saved to best_checkpoint_40994.pth (acc=0.9148, T=100, z=5)\n",
            "dim_hidden 500, latent_dim: 10\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.7011\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.6096\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.5775\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.5664\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.5788\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.5592\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.5249\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.5411\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.5107\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.4933\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "Epoch 100/1000, Loss: 0.9118, Recon-Loss: 0.8805, KL-Loss: 0.3127\n",
            "Epoch 200/1000, Loss: 0.8573, Recon-Loss: 0.8292, KL-Loss: 0.2813\n",
            "Epoch 300/1000, Loss: 0.8454, Recon-Loss: 0.8211, KL-Loss: 0.2431\n",
            "Epoch 400/1000, Loss: 0.8491, Recon-Loss: 0.8321, KL-Loss: 0.1703\n",
            "Epoch 500/1000, Loss: 0.8179, Recon-Loss: 0.7932, KL-Loss: 0.2479\n",
            "Epoch 600/1000, Loss: 0.8144, Recon-Loss: 0.7966, KL-Loss: 0.1778\n",
            "Epoch 700/1000, Loss: 0.7850, Recon-Loss: 0.7676, KL-Loss: 0.1741\n",
            "Epoch 800/1000, Loss: 0.7983, Recon-Loss: 0.7858, KL-Loss: 0.1244\n",
            "Epoch 900/1000, Loss: 0.8022, Recon-Loss: 0.7841, KL-Loss: 0.1816\n",
            "Epoch 1000/1000, Loss: 0.7576, Recon-Loss: 0.7371, KL-Loss: 0.2051\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.9148\n",
            "ARI: 0.0000\n",
            "Saved results to clutad0/clusterddpm_results.json\n",
            "\n",
            "dim_hidden 500, latent_dim: 15\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.6939\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.5277\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.5092\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.4769\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.4673\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.4154\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.3995\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.4143\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.3902\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.3803\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "Epoch 100/1000, Loss: 0.9149, Recon-Loss: 0.8800, KL-Loss: 0.3491\n",
            "Epoch 200/1000, Loss: 0.8533, Recon-Loss: 0.8497, KL-Loss: 0.0362\n",
            "Epoch 300/1000, Loss: 0.8445, Recon-Loss: 0.8431, KL-Loss: 0.0140\n",
            "Epoch 400/1000, Loss: 0.8289, Recon-Loss: 0.8282, KL-Loss: 0.0069\n",
            "Epoch 500/1000, Loss: 0.8043, Recon-Loss: 0.8038, KL-Loss: 0.0047\n",
            "Epoch 600/1000, Loss: 0.7821, Recon-Loss: 0.7813, KL-Loss: 0.0075\n",
            "Epoch 700/1000, Loss: 0.7882, Recon-Loss: 0.7880, KL-Loss: 0.0018\n",
            "Epoch 800/1000, Loss: 0.7589, Recon-Loss: 0.7585, KL-Loss: 0.0044\n",
            "Epoch 900/1000, Loss: 0.7732, Recon-Loss: 0.7727, KL-Loss: 0.0043\n",
            "Epoch 1000/1000, Loss: 0.7448, Recon-Loss: 0.7445, KL-Loss: 0.0035\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.9148\n",
            "ARI: 0.0000\n",
            "Saved results to clutad0/clusterddpm_results.json\n",
            "\n",
            "dim_hidden 500, latent_dim: 20\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.6185\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.5220\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.4501\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.4381\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.4043\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.4024\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.3640\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.3095\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.2876\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.2946\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "Epoch 100/1000, Loss: 0.8806, Recon-Loss: 0.8754, KL-Loss: 0.0523\n",
            "Epoch 200/1000, Loss: 0.8690, Recon-Loss: 0.8681, KL-Loss: 0.0087\n",
            "Epoch 300/1000, Loss: 0.8494, Recon-Loss: 0.8491, KL-Loss: 0.0025\n",
            "Epoch 400/1000, Loss: 0.8460, Recon-Loss: 0.8457, KL-Loss: 0.0024\n",
            "Epoch 500/1000, Loss: 0.8287, Recon-Loss: 0.8285, KL-Loss: 0.0021\n",
            "Epoch 600/1000, Loss: 0.8298, Recon-Loss: 0.8295, KL-Loss: 0.0031\n",
            "Epoch 700/1000, Loss: 0.7947, Recon-Loss: 0.7944, KL-Loss: 0.0023\n",
            "Epoch 800/1000, Loss: 0.7920, Recon-Loss: 0.7918, KL-Loss: 0.0020\n",
            "Epoch 900/1000, Loss: 0.7621, Recon-Loss: 0.7619, KL-Loss: 0.0019\n",
            "Epoch 1000/1000, Loss: 0.7530, Recon-Loss: 0.7527, KL-Loss: 0.0023\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.9148\n",
            "ARI: 0.0000\n",
            "Saved results to clutad0/clusterddpm_results.json\n",
            "\n",
            "dim_hidden 1000, latent_dim: 5\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.7365\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.7338\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.7291\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.7107\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.6681\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.6502\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.6717\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.6262\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.6178\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.6295\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "Epoch 100/1000, Loss: 0.8327, Recon-Loss: 0.8258, KL-Loss: 0.0689\n",
            "Epoch 200/1000, Loss: 0.7737, Recon-Loss: 0.7713, KL-Loss: 0.0247\n",
            "Epoch 300/1000, Loss: 0.7842, Recon-Loss: 0.7823, KL-Loss: 0.0192\n",
            "Epoch 400/1000, Loss: 0.7587, Recon-Loss: 0.7580, KL-Loss: 0.0069\n",
            "Epoch 500/1000, Loss: 0.7310, Recon-Loss: 0.7304, KL-Loss: 0.0056\n",
            "Epoch 600/1000, Loss: 0.7183, Recon-Loss: 0.7175, KL-Loss: 0.0076\n",
            "Epoch 700/1000, Loss: 0.6940, Recon-Loss: 0.6923, KL-Loss: 0.0168\n",
            "Epoch 800/1000, Loss: 0.6995, Recon-Loss: 0.6974, KL-Loss: 0.0208\n",
            "Epoch 900/1000, Loss: 0.6687, Recon-Loss: 0.6680, KL-Loss: 0.0065\n",
            "Epoch 1000/1000, Loss: 0.6551, Recon-Loss: 0.6539, KL-Loss: 0.0124\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.9148\n",
            "ARI: 0.0000\n",
            "Saved results to clutad0/clusterddpm_results.json\n",
            "\n",
            "dim_hidden 1000, latent_dim: 10\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.6626\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.5909\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.5888\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.5580\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.5531\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.5328\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.5273\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.4968\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.4692\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.4824\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "Epoch 100/1000, Loss: 0.8875, Recon-Loss: 0.8552, KL-Loss: 0.3224\n",
            "Epoch 200/1000, Loss: 0.8227, Recon-Loss: 0.8218, KL-Loss: 0.0089\n",
            "Epoch 300/1000, Loss: 0.8221, Recon-Loss: 0.8214, KL-Loss: 0.0062\n",
            "Epoch 400/1000, Loss: 0.7860, Recon-Loss: 0.7854, KL-Loss: 0.0056\n",
            "Epoch 500/1000, Loss: 0.7684, Recon-Loss: 0.7680, KL-Loss: 0.0041\n",
            "Epoch 600/1000, Loss: 0.7383, Recon-Loss: 0.7378, KL-Loss: 0.0049\n",
            "Epoch 700/1000, Loss: 0.7334, Recon-Loss: 0.7328, KL-Loss: 0.0058\n",
            "Epoch 800/1000, Loss: 0.7154, Recon-Loss: 0.7151, KL-Loss: 0.0028\n",
            "Epoch 900/1000, Loss: 0.7127, Recon-Loss: 0.7122, KL-Loss: 0.0051\n",
            "Epoch 1000/1000, Loss: 0.6880, Recon-Loss: 0.6877, KL-Loss: 0.0029\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.9148\n",
            "ARI: 0.0000\n",
            "Saved results to clutad0/clusterddpm_results.json\n",
            "\n",
            "dim_hidden 1000, latent_dim: 15\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.6249\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.5144\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.5118\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.4483\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.4425\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.4019\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.3670\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.3535\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.3684\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.3435\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "Epoch 100/1000, Loss: 0.8637, Recon-Loss: 0.8613, KL-Loss: 0.0242\n",
            "Epoch 200/1000, Loss: 0.8631, Recon-Loss: 0.8626, KL-Loss: 0.0058\n",
            "Epoch 300/1000, Loss: 0.8328, Recon-Loss: 0.8325, KL-Loss: 0.0033\n",
            "Epoch 400/1000, Loss: 0.8031, Recon-Loss: 0.8027, KL-Loss: 0.0041\n",
            "Epoch 500/1000, Loss: 0.7973, Recon-Loss: 0.7968, KL-Loss: 0.0044\n",
            "Epoch 600/1000, Loss: 0.7669, Recon-Loss: 0.7665, KL-Loss: 0.0039\n",
            "Epoch 700/1000, Loss: 0.7346, Recon-Loss: 0.7341, KL-Loss: 0.0052\n",
            "Epoch 800/1000, Loss: 0.7273, Recon-Loss: 0.7268, KL-Loss: 0.0058\n",
            "Epoch 900/1000, Loss: 0.6952, Recon-Loss: 0.6948, KL-Loss: 0.0033\n",
            "Epoch 1000/1000, Loss: 0.6785, Recon-Loss: 0.6781, KL-Loss: 0.0046\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.9148\n",
            "ARI: 0.0000\n",
            "Saved results to clutad0/clusterddpm_results.json\n",
            "\n",
            "dim_hidden 1000, latent_dim: 20\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.5676\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.5027\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.4058\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.3546\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.3684\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.3544\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.2968\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.3103\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.2635\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.2593\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "Epoch 100/1000, Loss: 0.8940, Recon-Loss: 0.8921, KL-Loss: 0.0191\n",
            "Epoch 200/1000, Loss: 0.8472, Recon-Loss: 0.8470, KL-Loss: 0.0022\n",
            "Epoch 300/1000, Loss: 0.8041, Recon-Loss: 0.8038, KL-Loss: 0.0030\n",
            "Epoch 400/1000, Loss: 0.8079, Recon-Loss: 0.8076, KL-Loss: 0.0031\n",
            "Epoch 500/1000, Loss: 0.8048, Recon-Loss: 0.8044, KL-Loss: 0.0044\n",
            "Epoch 600/1000, Loss: 0.7793, Recon-Loss: 0.7790, KL-Loss: 0.0033\n",
            "Epoch 700/1000, Loss: 0.7492, Recon-Loss: 0.7487, KL-Loss: 0.0042\n",
            "Epoch 800/1000, Loss: 0.7373, Recon-Loss: 0.7364, KL-Loss: 0.0089\n",
            "Epoch 900/1000, Loss: 0.6774, Recon-Loss: 0.6768, KL-Loss: 0.0060\n",
            "Epoch 1000/1000, Loss: 0.6559, Recon-Loss: 0.6554, KL-Loss: 0.0049\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.9148\n",
            "ARI: 0.0000\n",
            "Saved results to clutad0/clusterddpm_results.json\n",
            "\n",
            "🏁 Tuning finished.\n",
            "Best acc: 0.9148\n",
            "Best config: dim_hidden=500, latent_dim=5\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, adjusted_rand_score\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# Define cluster alignment function\n",
        "def cluster_accuracy(y_true, y_pred):\n",
        "    contingency = confusion_matrix(y_true, y_pred)\n",
        "    row_ind, col_ind = linear_sum_assignment(-contingency)\n",
        "    mapping = dict(zip(col_ind, row_ind))\n",
        "    y_aligned = np.array([mapping[label] for label in y_pred])\n",
        "    acc = accuracy_score(y_true, y_aligned)\n",
        "    return acc, y_aligned\n",
        "\n",
        "\n",
        "dataset_index = '40994'\n",
        "\n",
        "\n",
        "\n",
        "# Config\n",
        "DATA_PATH = 'data_processed.csv'\n",
        "CHECKPOINT_PATH_PRE = 'pretrain_checkpoint.pth'\n",
        "CHECKPOINT_PATH_FINAL = 'final_checkpoint.pth'\n",
        "LABEL_PATH = 'clusters.csv'\n",
        "METADATA_PATH = 'metadata.json'\n",
        "\n",
        "\n",
        "with open(METADATA_PATH, 'r') as f:\n",
        "  metadata = json.load(f)\n",
        "num_numeric = metadata['num_numerical_features']\n",
        "categories = metadata['num_classes_per_cat']\n",
        "n_clusters = metadata['num_clusters']\n",
        "\n",
        "T = 100 # this is a question !!!\n",
        "\n",
        "pretrain_steps = 1000 # same as in example\n",
        "em_epochs = 1000 # same as in example (it is 1000 altogether)\n",
        "batch_size = 256 # same as in example\n",
        "hidden_dims=[500, 500, 2000] # same as in example\n",
        "kl_weight=0.1 # same as in example\n",
        "lr = 1e-3 # same as in example\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "x_real = torch.tensor(df.values, dtype=torch.float32).to(device)\n",
        "dataset = TensorDataset(x_real)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "# Load ground truth\n",
        "y_true = pd.read_csv(LABEL_PATH).values.flatten()\n",
        "if y_true.dtype.kind in {'U', 'S', 'O'}:\n",
        "    unique_labels, y_true = np.unique(np.asarray(y_true).astype(str), return_inverse=True)\n",
        "N, D = x_real.shape\n",
        "\n",
        "BEST_PATH = f\"best_checkpoint_{dataset_index}.pth\"\n",
        "best_acc = -1.0\n",
        "best_meta = None\n",
        "for dim_hidden in [500, 1000]:\n",
        "  for latent_dim in [5, 10, 15, 20]:\n",
        "    print(f\"dim_hidden {dim_hidden}, latent_dim: {latent_dim}\")\n",
        "\n",
        "    # Models\n",
        "    encoder = TabularEncoder(\n",
        "        input_dim=D,\n",
        "        hidden_dims=hidden_dims,\n",
        "        latent_dim=latent_dim\n",
        "    ).to(device)\n",
        "\n",
        "    denoiser = Denoiser(\n",
        "        dim_in=D,\n",
        "        latent_dim=latent_dim,\n",
        "        dim_hidden=dim_hidden,\n",
        "        num_numeric=num_numeric,\n",
        "        categories=categories\n",
        "    ).to(device)\n",
        "\n",
        "    # ClusterDDPM wrapper\n",
        "    model = ClusterDDPM(\n",
        "        encoder=encoder,\n",
        "        denoiser=denoiser,\n",
        "        T=T,\n",
        "        num_numeric=num_numeric,\n",
        "        categories=categories,\n",
        "        n_clusters=n_clusters,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(\n",
        "        list(encoder.parameters()) + list(denoiser.parameters()),\n",
        "        lr=lr, weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "    # 🔹 Pretraining\n",
        "    print(\"🔹 Starting pretraining...\")\n",
        "    model.pretrain(dataloader, optimizer, epochs=pretrain_steps, batch_size=batch_size, plot_freq=100)\n",
        "\n",
        "    # Save pretraining checkpoint\n",
        "    #os.makedirs(os.path.dirname(CHECKPOINT_PATH_PRE), exist_ok=True)\n",
        "    #torch.save({\n",
        "    #    'encoder': encoder.state_dict(),\n",
        "    #    'denoiser': denoiser.state_dict(),\n",
        "    #    'optimizer': optimizer.state_dict(),\n",
        "    #    'T': T,\n",
        "    #    'num_numeric': num_numeric,\n",
        "    #    'categories': categories\n",
        "    #}, CHECKPOINT_PATH_PRE)\n",
        "    #print(f\"✅ Pretraining checkpoint saved at {CHECKPOINT_PATH_PRE}\")\n",
        "\n",
        "    # 🔹 Fit initial GMM (E-step 0)\n",
        "    print(\"🔹 Fitting initial GMM...\")\n",
        "    model.fit_gmm(dataloader)\n",
        "\n",
        "    # 🔹 EM training loop\n",
        "    print(\"🔹 Starting EM training...\")\n",
        "    for epoch in range(em_epochs):\n",
        "        # M-step\n",
        "        # Optimizer\n",
        "        optimizer = optim.Adam(\n",
        "            list(encoder.parameters()) + list(denoiser.parameters()),\n",
        "            lr=lr, weight_decay=1e-4\n",
        "        )\n",
        "        avg_loss, avg_recon_loss, avg_kl_loss = model.train_elbo(dataloader, optimizer, batch_size=batch_size, kl_weight=kl_weight, plot_freq=50)\n",
        "        if (epoch+1) % 100 == 0:\n",
        "          print(f'Epoch {epoch+1}/{em_epochs}, Loss: {avg_loss:.4f}, Recon-Loss: {avg_recon_loss:.4f}, KL-Loss: {avg_kl_loss:.4f}')\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          # E-step\n",
        "          model.fit_gmm(dataloader)\n",
        "\n",
        "        #with torch.no_grad():\n",
        "          # mu, logvar = model.encoder(x_real)\n",
        "          #z_np = mu.cpu().numpy()\n",
        "          #y_pred = model.gmm.predict(z_np)\n",
        "\n",
        "          # Compute metrics\n",
        "          #accuracy, y_aligned = cluster_accuracy(y_true, y_pred)\n",
        "          #print(y_pred)\n",
        "          #print(accuracy)\n",
        "    # Save final model\n",
        "    #os.makedirs(os.path.dirname(CHECKPOINT_PATH_FINAL), exist_ok=True)\n",
        "    #torch.save({\n",
        "    #    'encoder': encoder.state_dict(),\n",
        "    #    'denoiser': denoiser.state_dict(),\n",
        "    #    'optimizer': optimizer.state_dict(),\n",
        "    #    'gmm': model.gmm\n",
        "    #}, CHECKPOINT_PATH_FINAL)\n",
        "    #print(f\"✅ Final checkpoint saved at {CHECKPOINT_PATH_FINAL}\")\n",
        "\n",
        "\n",
        "    # Encode all data and compute GMM assignments\n",
        "    with torch.no_grad():\n",
        "        mu, logvar = model.encoder(x_real)\n",
        "        z_np = mu.cpu().numpy()\n",
        "        y_pred = model.gmm.predict(z_np)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy, y_aligned = cluster_accuracy(y_true, y_pred)\n",
        "    ari = adjusted_rand_score(y_true, y_pred)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"✅ Final clustering performance:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"ARI: {ari:.4f}\")\n",
        "\n",
        "\n",
        "    # Example: after you finish one training run\n",
        "    results = {\n",
        "        \"accuracy\": float(accuracy),      # your computed accuracy\n",
        "        \"ari\": float(ari),                # your computed ARI\n",
        "        \"T\": int(T),                      # diffusion timesteps\n",
        "        \"dim_hidden\": int(dim_hidden),    # hidden dimension\n",
        "        \"latent_dim\": int(latent_dim),    # latent dimension\n",
        "        \"dataset_index\": str(dataset_index)      # e.g., \"mnist\", \"cifar10\", etc.\n",
        "    }\n",
        "\n",
        "    # Path to results file\n",
        "    results_file = Path(\"clutad0/clusterddpm_results.json\")\n",
        "\n",
        "    # If file exists, load and append; else create new\n",
        "    if results_file.exists():\n",
        "        with open(results_file, \"r\") as f:\n",
        "            all_results = json.load(f)\n",
        "    else:\n",
        "        all_results = []\n",
        "\n",
        "    all_results.append(results)\n",
        "\n",
        "    # Save updated results\n",
        "    with open(results_file, \"w\") as f:\n",
        "        json.dump(all_results, f, indent=4)\n",
        "\n",
        "    print(f\"Saved results to {results_file}\\n\")\n",
        "\n",
        "    # --- NEW: update 'best' and save checkpoint if improved ---\n",
        "    if accuracy > best_acc:\n",
        "        best_acc = float(accuracy)\n",
        "        best_meta = {\n",
        "            \"T\": int(T),\n",
        "            \"latent_dim\": int(latent_dim),\n",
        "            \"dim_hidden\": int(dim_hidden),\n",
        "            \"dataset_index\": str(dataset_index),\n",
        "            \"accuracy\": float(accuracy),\n",
        "            \"ari\": float(ari)\n",
        "        }\n",
        "\n",
        "        # save immediately so you don't lose it if the script stops\n",
        "        torch.save({\n",
        "            \"encoder\": encoder.state_dict(),\n",
        "            \"denoiser\": denoiser.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),  # optional but handy\n",
        "            \"gmm\": model.gmm,                     # sklearn object; torch.save pickles it\n",
        "            \"config\": {\n",
        "                \"T\": T,\n",
        "                \"num_numeric\": num_numeric,\n",
        "                \"categories\": categories,\n",
        "                \"n_clusters\": n_clusters,\n",
        "                \"hidden_dims\": hidden_dims,\n",
        "                \"dim_hidden\": dim_hidden,\n",
        "                \"latent_dim\": latent_dim,\n",
        "                \"kl_weight\": kl_weight,\n",
        "                \"lr\": lr,\n",
        "                \"batch_size\": batch_size,\n",
        "                \"em_epochs\": em_epochs,\n",
        "                \"pretrain_steps\": pretrain_steps,\n",
        "            },\n",
        "            \"metrics\": {\n",
        "                \"accuracy\": float(accuracy),\n",
        "                \"ari\": float(ari)\n",
        "            }\n",
        "        }, BEST_PATH)\n",
        "\n",
        "        print(f\"💾 New best model saved to {BEST_PATH} (acc={accuracy:.4f}, T={T}, z={latent_dim})\")\n",
        "    # ----------------------------------------------------------\n",
        "\n",
        "# --- after both loops finish ---\n",
        "print(\"🏁 Tuning finished.\")\n",
        "print(f\"Best acc: {best_acc:.4f}\")\n",
        "if best_meta is not None:\n",
        "    print(f\"Best config: dim_hidden={best_meta['dim_hidden']}, latent_dim={best_meta['latent_dim']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
