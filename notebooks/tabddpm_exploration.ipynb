{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a13c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15cca6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V10</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.190940</td>\n",
       "      <td>-0.151318</td>\n",
       "      <td>-1.016737</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-0.444948</td>\n",
       "      <td>-1.411573</td>\n",
       "      <td>-1.350125</td>\n",
       "      <td>0.188461</td>\n",
       "      <td>0.212631</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.020355</td>\n",
       "      <td>-0.645041</td>\n",
       "      <td>1.405765</td>\n",
       "      <td>1.423335</td>\n",
       "      <td>1.646522</td>\n",
       "      <td>0.740885</td>\n",
       "      <td>0.801780</td>\n",
       "      <td>0.970866</td>\n",
       "      <td>0.110048</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.020355</td>\n",
       "      <td>-0.283710</td>\n",
       "      <td>1.235902</td>\n",
       "      <td>1.288435</td>\n",
       "      <td>1.231294</td>\n",
       "      <td>0.655684</td>\n",
       "      <td>0.493028</td>\n",
       "      <td>0.440198</td>\n",
       "      <td>0.212631</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.707298</td>\n",
       "      <td>0.270284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116550</td>\n",
       "      <td>-0.534807</td>\n",
       "      <td>-1.746316</td>\n",
       "      <td>-1.144112</td>\n",
       "      <td>0.188461</td>\n",
       "      <td>0.306203</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.672115</td>\n",
       "      <td>-1.991811</td>\n",
       "      <td>0.930265</td>\n",
       "      <td>0.926951</td>\n",
       "      <td>-0.245794</td>\n",
       "      <td>-0.400208</td>\n",
       "      <td>0.376984</td>\n",
       "      <td>0.792902</td>\n",
       "      <td>-0.907275</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1       V10        V3        V4        V5        V6        V7  \\\n",
       "0  1.190940 -0.151318 -1.016737 -5.199338 -0.444948 -1.411573 -1.350125   \n",
       "1  1.020355 -0.645041  1.405765  1.423335  1.646522  0.740885  0.801780   \n",
       "2  1.020355 -0.283710  1.235902  1.288435  1.231294  0.655684  0.493028   \n",
       "3  0.707298  0.270284  0.000000  0.116550 -0.534807 -1.746316 -1.144112   \n",
       "4  1.672115 -1.991811  0.930265  0.926951 -0.245794 -0.400208  0.376984   \n",
       "\n",
       "         V8        V9   V2  \n",
       "0  0.188461  0.212631  0.0  \n",
       "1  0.970866  0.110048  1.0  \n",
       "2  0.440198  0.212631  1.0  \n",
       "3  0.188461  0.306203  1.0  \n",
       "4  0.792902 -0.907275  1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'data/preprocessed/1480/data_processed.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()\n",
    "\n",
    "X = torch.tensor(df.values, dtype=torch.float32)\n",
    "batch_size = 264\n",
    "data_loader = DataLoader(TensorDataset(X), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "68ab466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from src.tabddpm.modules import timestep_embedding, MLP\n",
    "\n",
    "class MLPDiffusion(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified MLP-based diffusion model for tabular data.\n",
    "\n",
    "    This model uses timestep embeddings and a projection layer\n",
    "    to inject time information into the input before processing it\n",
    "    through an MLP. It is intended for unsupervised tasks like clustering,\n",
    "    where label conditioning is not required.\n",
    "\n",
    "    Args:\n",
    "        d_in (int): Input feature dimension.\n",
    "        rtdl_params (dict): Parameters for the RTDL MLP (e.g., hidden sizes, depth).\n",
    "        dim_t (int): Dimensionality for the timestep embedding and projection space.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, d_layers, dropout, d_t):\n",
    "        super().__init__()\n",
    "        self.dim_t = d_t\n",
    "\n",
    "        # Configure MLP: input will be timestep-embedded, output must match original input size\n",
    "        self.mlp = MLP.make_baseline(d_t, d_layers, dropout, d_in)\n",
    "\n",
    "        # Project input features into timestep embedding space\n",
    "        self.proj = nn.Linear(d_in, d_t)\n",
    "\n",
    "        # Timestep embedding network (2-layer MLP with SiLU activation)\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(d_t, d_t),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_t, d_t)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        \"\"\"\n",
    "        Forward pass of the MLPDiffusion model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, d_in).\n",
    "            timesteps (Tensor): Timestep tensor of shape (batch_size,) representing the diffusion step.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, d_in), same as the input dimension.\n",
    "        \"\"\"\n",
    "        # Get timestep embedding (e.g., sinusoidal), then pass through a small MLP\n",
    "        emb = self.time_embed(timestep_embedding(timesteps, self.dim_t))\n",
    "\n",
    "        # Project input to timestep embedding space and add time information\n",
    "        x = self.proj(x) + emb\n",
    "\n",
    "        # Pass through MLP and return\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "1c7af97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tabddpm.gaussian_multinomial_diffusion import GaussianMultinomialDiffusion\n",
    "\n",
    "def get_diffusion_model(model, num_classes, num_numerical, device):\n",
    "    diffusion = GaussianMultinomialDiffusion(\n",
    "        num_classes=np.array(num_classes),\n",
    "        num_numerical_features=num_numerical,\n",
    "        denoise_fn=model,\n",
    "        gaussian_loss_type='mse',\n",
    "        num_timesteps=1000,\n",
    "        scheduler='cosine',\n",
    "        device=device\n",
    "    )\n",
    "    diffusion.to(device)\n",
    "    return diffusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "dd29002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch.optim as optim\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, steps=1000, lr=1e-3, device='cuda'):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.steps = steps\n",
    "        self.device = device\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "        self.ema_model = deepcopy(model._denoise_fn)\n",
    "        for p in self.ema_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def train(self):\n",
    "        step = 0\n",
    "        iterator = iter(self.train_loader)\n",
    "\n",
    "        while step < self.steps:\n",
    "            try:\n",
    "                x_batch, = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(self.train_loader)\n",
    "                x_batch, = next(iterator)\n",
    "\n",
    "            x_batch = x_batch.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss_multi, loss_gauss = self.model.mixed_loss(x_batch, {})\n",
    "            loss = loss_multi + loss_gauss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"[{step}/{self.steps}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "e1c24096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\SpecProF\\Python\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:2829: UserWarning: n_quantiles (1000) is greater than the total number of samples (583). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10000] Loss: 1.6829\n",
      "[100/10000] Loss: 1.4561\n",
      "[200/10000] Loss: 0.8749\n",
      "[300/10000] Loss: 0.9362\n",
      "[400/10000] Loss: 1.1499\n",
      "[500/10000] Loss: 1.0311\n",
      "[600/10000] Loss: 1.1920\n",
      "[700/10000] Loss: 0.8606\n",
      "[800/10000] Loss: 0.8757\n",
      "[900/10000] Loss: 0.9447\n",
      "[1000/10000] Loss: 0.7220\n",
      "[1100/10000] Loss: 0.7616\n",
      "[1200/10000] Loss: 1.1235\n",
      "[1300/10000] Loss: 0.7658\n",
      "[1400/10000] Loss: 1.0784\n",
      "[1500/10000] Loss: 0.8105\n",
      "[1600/10000] Loss: 0.9295\n",
      "[1700/10000] Loss: 0.9923\n",
      "[1800/10000] Loss: 0.9349\n",
      "[1900/10000] Loss: 0.8035\n",
      "[2000/10000] Loss: 0.7384\n",
      "[2100/10000] Loss: 0.9547\n",
      "[2200/10000] Loss: 0.8283\n",
      "[2300/10000] Loss: 0.7792\n",
      "[2400/10000] Loss: 0.7851\n",
      "[2500/10000] Loss: 1.5325\n",
      "[2600/10000] Loss: 1.1241\n",
      "[2700/10000] Loss: 0.8784\n",
      "[2800/10000] Loss: 0.9501\n",
      "[2900/10000] Loss: 0.8155\n",
      "[3000/10000] Loss: 0.8488\n",
      "[3100/10000] Loss: 0.9240\n",
      "[3200/10000] Loss: 0.7233\n",
      "[3300/10000] Loss: 0.9234\n",
      "[3400/10000] Loss: 0.9506\n",
      "[3500/10000] Loss: 0.8120\n",
      "[3600/10000] Loss: 0.8311\n",
      "[3700/10000] Loss: 0.7130\n",
      "[3800/10000] Loss: 1.1741\n",
      "[3900/10000] Loss: 0.7583\n",
      "[4000/10000] Loss: 1.0325\n",
      "[4100/10000] Loss: 0.6520\n",
      "[4200/10000] Loss: 0.7610\n",
      "[4300/10000] Loss: 0.8483\n",
      "[4400/10000] Loss: 0.8603\n",
      "[4500/10000] Loss: 0.9523\n",
      "[4600/10000] Loss: 0.9037\n",
      "[4700/10000] Loss: 0.8358\n",
      "[4800/10000] Loss: 0.8117\n",
      "[4900/10000] Loss: 0.8213\n",
      "[5000/10000] Loss: 0.6856\n",
      "[5100/10000] Loss: 0.7748\n",
      "[5200/10000] Loss: 0.9612\n",
      "[5300/10000] Loss: 0.8671\n",
      "[5400/10000] Loss: 0.8277\n",
      "[5500/10000] Loss: 0.8829\n",
      "[5600/10000] Loss: 0.8720\n",
      "[5700/10000] Loss: 0.7514\n",
      "[5800/10000] Loss: 0.7659\n",
      "[5900/10000] Loss: 0.7763\n",
      "[6000/10000] Loss: 0.8074\n",
      "[6100/10000] Loss: 0.8642\n",
      "[6200/10000] Loss: 0.9730\n",
      "[6300/10000] Loss: 1.3668\n",
      "[6400/10000] Loss: 0.9842\n",
      "[6500/10000] Loss: 0.8132\n",
      "[6600/10000] Loss: 0.7615\n",
      "[6700/10000] Loss: 0.9498\n",
      "[6800/10000] Loss: 0.8727\n",
      "[6900/10000] Loss: 0.8087\n",
      "[7000/10000] Loss: 0.9517\n",
      "[7100/10000] Loss: 0.8415\n",
      "[7200/10000] Loss: 0.8454\n",
      "[7300/10000] Loss: 1.1348\n",
      "[7400/10000] Loss: 0.8350\n",
      "[7500/10000] Loss: 0.6882\n",
      "[7600/10000] Loss: 0.7881\n",
      "[7700/10000] Loss: 0.7259\n",
      "[7800/10000] Loss: 0.6982\n",
      "[7900/10000] Loss: 0.8013\n",
      "[8000/10000] Loss: 0.8116\n",
      "[8100/10000] Loss: 0.6877\n",
      "[8200/10000] Loss: 0.9829\n",
      "[8300/10000] Loss: 0.7422\n",
      "[8400/10000] Loss: 0.7341\n",
      "[8500/10000] Loss: 0.8429\n",
      "[8600/10000] Loss: 1.0538\n",
      "[8700/10000] Loss: 0.7368\n",
      "[8800/10000] Loss: 0.8245\n",
      "[8900/10000] Loss: 0.6809\n",
      "[9000/10000] Loss: 0.7577\n",
      "[9100/10000] Loss: 0.6900\n",
      "[9200/10000] Loss: 0.6609\n",
      "[9300/10000] Loss: 1.0525\n",
      "[9400/10000] Loss: 0.9185\n",
      "[9500/10000] Loss: 0.6822\n",
      "[9600/10000] Loss: 0.7542\n",
      "[9700/10000] Loss: 0.7668\n",
      "[9800/10000] Loss: 0.9307\n",
      "[9900/10000] Loss: 0.7323\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 256\n",
    "steps = 10000\n",
    "lr = 0.001\n",
    "\n",
    "d_layers = [256, 256, 256]\n",
    "dropout = 0.0\n",
    "d_t = 128\n",
    "\n",
    "data_loader, num_numerical, num_classes = prepare_dataset(df_train, ['V2'], batch_size)\n",
    "\n",
    "\n",
    "mlp_model = MLPDiffusion(\n",
    "    d_in=num_numerical + sum(num_classes),\n",
    "    d_layers=d_layers,\n",
    "    dropout=dropout,\n",
    "    d_t=d_t\n",
    "    )\n",
    "\n",
    "mlp_model.to(device)\n",
    "\n",
    "diffusion = get_diffusion_model(mlp_model, num_classes, num_numerical, device)\n",
    "\n",
    "trainer = Trainer(diffusion, data_loader, steps=steps, device=device, lr=lr)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "73dcd496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with d_layers=[128, 128], dropout=0.0, d_t=64\n",
      "Avg Loss: 2.5348\n",
      "\n",
      "Training with d_layers=[128, 128], dropout=0.0, d_t=128\n",
      "Avg Loss: 3.1556\n",
      "\n",
      "Training with d_layers=[128, 128], dropout=0.0, d_t=256\n",
      "Avg Loss: 2.3861\n",
      "\n",
      "Training with d_layers=[128, 128], dropout=0.1, d_t=64\n",
      "Avg Loss: 3.4934\n",
      "\n",
      "Training with d_layers=[128, 128], dropout=0.1, d_t=128\n",
      "Avg Loss: 4.5561\n",
      "\n",
      "Training with d_layers=[128, 128], dropout=0.1, d_t=256\n",
      "Avg Loss: 2.8527\n",
      "\n",
      "Training with d_layers=[128, 128], dropout=0.2, d_t=64\n",
      "Avg Loss: 3.7634\n",
      "\n",
      "Training with d_layers=[128, 128], dropout=0.2, d_t=128\n",
      "Avg Loss: 4.5156\n",
      "\n",
      "Training with d_layers=[128, 128], dropout=0.2, d_t=256\n",
      "Avg Loss: 3.5587\n",
      "\n",
      "Training with d_layers=[256, 256], dropout=0.0, d_t=64\n",
      "Avg Loss: 2.8938\n",
      "\n",
      "Training with d_layers=[256, 256], dropout=0.0, d_t=128\n",
      "Avg Loss: 2.3748\n",
      "\n",
      "Training with d_layers=[256, 256], dropout=0.0, d_t=256\n",
      "Avg Loss: 2.6721\n",
      "\n",
      "Training with d_layers=[256, 256], dropout=0.1, d_t=64\n",
      "Avg Loss: 3.0296\n",
      "\n",
      "Training with d_layers=[256, 256], dropout=0.1, d_t=128\n",
      "Avg Loss: 2.8752\n",
      "\n",
      "Training with d_layers=[256, 256], dropout=0.1, d_t=256\n",
      "Avg Loss: 2.5781\n",
      "\n",
      "Training with d_layers=[256, 256], dropout=0.2, d_t=64\n",
      "Avg Loss: 4.5304\n",
      "\n",
      "Training with d_layers=[256, 256], dropout=0.2, d_t=128\n",
      "Avg Loss: 3.3894\n",
      "\n",
      "Training with d_layers=[256, 256], dropout=0.2, d_t=256\n",
      "Avg Loss: 3.0771\n",
      "\n",
      "Training with d_layers=[256, 256, 256], dropout=0.0, d_t=64\n",
      "Avg Loss: 1.8388\n",
      "\n",
      "Training with d_layers=[256, 256, 256], dropout=0.0, d_t=128\n",
      "Avg Loss: 1.5868\n",
      "\n",
      "Training with d_layers=[256, 256, 256], dropout=0.0, d_t=256\n",
      "Avg Loss: 2.2676\n",
      "\n",
      "Training with d_layers=[256, 256, 256], dropout=0.1, d_t=64\n",
      "Avg Loss: 2.1845\n",
      "\n",
      "Training with d_layers=[256, 256, 256], dropout=0.1, d_t=128\n",
      "Avg Loss: 2.2844\n",
      "\n",
      "Training with d_layers=[256, 256, 256], dropout=0.1, d_t=256\n",
      "Avg Loss: 2.0063\n",
      "\n",
      "Training with d_layers=[256, 256, 256], dropout=0.2, d_t=64\n",
      "Avg Loss: 2.1763\n",
      "\n",
      "Training with d_layers=[256, 256, 256], dropout=0.2, d_t=128\n",
      "Avg Loss: 2.1177\n",
      "\n",
      "Training with d_layers=[256, 256, 256], dropout=0.2, d_t=256\n",
      "Avg Loss: 1.8648\n",
      "\n",
      "Training with d_layers=[512, 256, 128], dropout=0.0, d_t=64\n",
      "Avg Loss: 1.9270\n",
      "\n",
      "Training with d_layers=[512, 256, 128], dropout=0.0, d_t=128\n",
      "Avg Loss: 1.7948\n",
      "\n",
      "Training with d_layers=[512, 256, 128], dropout=0.0, d_t=256\n",
      "Avg Loss: 1.7980\n",
      "\n",
      "Training with d_layers=[512, 256, 128], dropout=0.1, d_t=64\n",
      "Avg Loss: 1.9063\n",
      "\n",
      "Training with d_layers=[512, 256, 128], dropout=0.1, d_t=128\n",
      "Avg Loss: 1.9169\n",
      "\n",
      "Training with d_layers=[512, 256, 128], dropout=0.1, d_t=256\n",
      "Avg Loss: 2.1688\n",
      "\n",
      "Training with d_layers=[512, 256, 128], dropout=0.2, d_t=64\n",
      "Avg Loss: 2.2825\n",
      "\n",
      "Training with d_layers=[512, 256, 128], dropout=0.2, d_t=128\n",
      "Avg Loss: 1.9666\n",
      "\n",
      "Training with d_layers=[512, 256, 128], dropout=0.2, d_t=256\n",
      "Avg Loss: 2.1886\n",
      "\n",
      "Best configuration:\n",
      "Layers: [256, 256, 256], Dropout: 0.0, d_t: 128\n",
      "Best loss: 1.5868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def train_and_eval(d_layers, dropout, d_t, data_loader, num_numerical, num_classes, device, steps=1000, lr=1e-3):\n",
    "    mlp_model = MLPDiffusion(\n",
    "        d_in=num_numerical + sum(num_classes),\n",
    "        d_layers=d_layers,\n",
    "        dropout=dropout,\n",
    "        d_t=d_t\n",
    "    )\n",
    "    mlp_model.to(device)\n",
    "\n",
    "    diffusion = get_diffusion_model(mlp_model, num_classes, num_numerical, device)\n",
    "    diffusion.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(diffusion.parameters(), lr=lr)\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for step, (x_batch, ) in enumerate(data_loader):\n",
    "        if step >= steps:\n",
    "            break\n",
    "\n",
    "        x_batch = x_batch.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        loss_multi, loss_gauss = diffusion.mixed_loss(x_batch, {})\n",
    "        loss = loss_multi + loss_gauss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(x_batch)\n",
    "        count += len(x_batch)\n",
    "\n",
    "    avg_loss = total_loss / count\n",
    "    return avg_loss, copy.deepcopy(diffusion.state_dict())\n",
    "\n",
    "# Hyperparameter search space\n",
    "hidden_layer_options = [\n",
    "    [128, 128],\n",
    "    [256, 256],\n",
    "    [256, 256, 256],\n",
    "    [512, 256, 128]\n",
    "]\n",
    "dropout_options = [0.0, 0.1, 0.2]\n",
    "d_t_options = [64, 128, 256]\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "best_state = None\n",
    "\n",
    "for d_layers in hidden_layer_options:\n",
    "    for dropout in dropout_options:\n",
    "        for d_t in d_t_options:\n",
    "            print(f\"Training with d_layers={d_layers}, dropout={dropout}, d_t={d_t}\")\n",
    "            avg_loss, state = train_and_eval(\n",
    "                d_layers=d_layers,\n",
    "                dropout=dropout,\n",
    "                d_t=d_t,\n",
    "                data_loader=data_loader,\n",
    "                num_numerical=num_numerical,\n",
    "                num_classes=num_classes,\n",
    "                device=device,\n",
    "                steps=1000,\n",
    "                lr=0.0001\n",
    "            )\n",
    "            print(f\"Avg Loss: {avg_loss:.4f}\\n\")\n",
    "\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_params = (d_layers, dropout, d_t)\n",
    "                best_state = state\n",
    "\n",
    "print(\"Best configuration:\")\n",
    "print(f\"Layers: {best_params[0]}, Dropout: {best_params[1]}, d_t: {best_params[2]}\")\n",
    "print(f\"Best loss: {best_loss:.4f}\")\n",
    "\n",
    "# To reload best model later:\n",
    "best_model = MLPDiffusion(\n",
    "    d_in=num_numerical + sum(num_classes),\n",
    "    d_layers=best_params[0],\n",
    "    dropout=best_params[1],\n",
    "    d_t=best_params[2]\n",
    ").to(device)\n",
    "mlp_state_dict = {k[len('_denoise_fn.'):]: v for k, v in best_state.items() if k.startswith('_denoise_fn.')}\n",
    "\n",
    "# Now load only the MLPDiffusion weights\n",
    "best_model.load_state_dict(mlp_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "c5d0fce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5333072059559372"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_loss, state = train_and_eval(\n",
    "    d_layers=[256,256,256],\n",
    "    dropout=0.0,\n",
    "    d_t=128,\n",
    "    data_loader=data_loader,\n",
    "    num_numerical=9,\n",
    "    num_classes=[2],\n",
    "    device=device,\n",
    "    steps=10000,\n",
    "    lr=0.0001\n",
    ")\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db8292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2012, -0.3061, -0.3158,  ..., -1.1850, -1.0592,  1.0000],\n",
      "        [ 0.9430,  1.2413,  1.2166,  ..., -1.3108, -1.0906,  1.0000],\n",
      "        [-1.7153, -0.4511, -0.4940,  ...,  1.7087,  2.3661,  0.0000],\n",
      "        ...,\n",
      "        [-1.1589, -0.4350, -0.4583,  ..., -0.9333, -0.6193,  0.0000],\n",
      "        [-0.7880,  3.1271,  3.1054,  ..., -0.4301, -0.7764,  1.0000],\n",
      "        [-0.2934, -0.4189, -0.4940,  ..., -0.6817,  0.1664,  1.0000]])\n",
      "tensor([[ 0.0989,  0.5668, -0.5592,  ..., -0.7411, -0.5338,  0.5286],\n",
      "        [ 0.0085,  0.2609, -0.0081,  ...,  0.3516, -1.7337,  1.1427],\n",
      "        [-0.1041, -0.1533,  0.2487,  ...,  0.0658, -0.6076,  0.5510],\n",
      "        ...,\n",
      "        [-1.0366, -0.3861,  0.4864,  ..., -0.6828, -0.6099,  0.3700],\n",
      "        [ 0.1357,  0.5098, -0.1787,  ..., -0.4231, -1.6590,  2.2879],\n",
      "        [ 0.1465, -1.6179,  0.8162,  ..., -0.4069, -0.4884,  0.5847]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def index_to_log_onehot(x, num_classes):\n",
    "    onehots = []\n",
    "    for i in range(len(num_classes)):\n",
    "        onehots.append(F.one_hot(x[:, i], num_classes[i]))\n",
    " \n",
    "    x_onehot = torch.cat(onehots, dim=1)\n",
    "    log_onehot = torch.log(x_onehot.float().clamp(min=1e-30))\n",
    "    return log_onehot\n",
    "\n",
    "x = next(iter(data_loader))[0]\n",
    "b = x.shape[0]\n",
    "t, pt = diffusion.sample_time(b, device, 'uniform')\n",
    "\n",
    "x_num = x[:, :diffusion.num_numerical_features]\n",
    "x_cat = x[:, diffusion.num_numerical_features:]\n",
    "\n",
    "x_num_t = x_num\n",
    "log_x_cat_t = x_cat\n",
    "if x_num.shape[1] > 0:\n",
    "    noise = torch.randn_like(x_num)\n",
    "    x_num_t = diffusion.gaussian_q_sample(x_num, t, noise=noise)\n",
    "if x_cat.shape[1] > 0:\n",
    "    log_x_cat = index_to_log_onehot(x_cat.long(), diffusion.num_classes)\n",
    "    log_x_cat_t = diffusion.q_sample(log_x_start=log_x_cat, t=t)\n",
    "\n",
    "x_in = torch.cat([x_num_t, log_x_cat_t], dim=1)\n",
    "\n",
    "model_out = diffusion._denoise_fn(\n",
    "    x_in,\n",
    "    t\n",
    ")\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(model_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b79d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mixed_loss(dm, data_loader, device):\n",
    "    dm.eval()\n",
    "    total_loss_multi = 0.0\n",
    "    total_loss_gauss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, in data_loader:\n",
    "            x_batch = batch.to(device).float()  # adjust if your loader returns tuple, etc.\n",
    "            b = x_batch.size(0)\n",
    "            total_samples += b\n",
    "\n",
    "            # empty dict if no conditions, else pass your dict\n",
    "            out_dict = {}\n",
    "\n",
    "            loss_multi, loss_gauss = dm.mixed_loss(x_batch, out_dict)\n",
    "\n",
    "            # loss_multi and loss_gauss are mean per batch, multiply by batch size for sum\n",
    "            total_loss_multi += loss_multi.item() * b\n",
    "            total_loss_gauss += loss_gauss.item() * b\n",
    "\n",
    "    return (total_loss_multi + total_loss_gauss) / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a5e31109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mixed_loss(dm, x, out_dict):\n",
    "    b = x.shape[0]\n",
    "    device = x.device\n",
    "    t, pt = dm.sample_time(b, device, 'uniform')\n",
    "\n",
    "    x_num = x[:, :dm.num_numerical_features]\n",
    "    x_cat = x[:, dm.num_numerical_features:]\n",
    "    \n",
    "    x_num_t = x_num\n",
    "    log_x_cat_t = x_cat\n",
    "    if x_num.shape[1] > 0:\n",
    "        noise = torch.randn_like(x_num)\n",
    "        x_num_t = dm.gaussian_q_sample(x_num, t, noise=noise)\n",
    "    if x_cat.shape[1] > 0:\n",
    "        log_x_cat = index_to_log_onehot(x_cat.long(), dm.num_classes)\n",
    "        log_x_cat_t = dm.q_sample(log_x_start=log_x_cat, t=t)\n",
    "    \n",
    "    x_in = torch.cat([x_num_t, log_x_cat_t], dim=1)\n",
    "\n",
    "    model_out = torch.randn_like(x_in)\n",
    "\n",
    "    model_out_num = model_out[:, :dm.num_numerical_features]\n",
    "    model_out_cat = model_out[:, dm.num_numerical_features:]\n",
    "\n",
    "    loss_multi = torch.zeros((1,)).float()\n",
    "    loss_gauss = torch.zeros((1,)).float()\n",
    "    if x_cat.shape[1] > 0:\n",
    "        loss_multi = dm._multinomial_loss(model_out_cat, log_x_cat, log_x_cat_t, t, pt, out_dict) / len(dm.num_classes)\n",
    "    \n",
    "    if x_num.shape[1] > 0:\n",
    "        loss_gauss = dm._gaussian_loss(model_out_num, x_num, x_num_t, t, noise)\n",
    "\n",
    "    # loss_multi = torch.where(out_dict['y'] == 1, loss_multi, 2 * loss_multi)\n",
    "    # loss_gauss = torch.where(out_dict['y'] == 1, loss_gauss, 2 * loss_gauss)\n",
    "\n",
    "    return loss_multi.mean(), loss_gauss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ab307d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random_mixed_loss(dm, data_loader, device):\n",
    "    dm.eval()\n",
    "    total_loss_multi = 0.0\n",
    "    total_loss_gauss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, in data_loader:\n",
    "            x_batch = batch.to(device).float()  # adjust if your loader returns tuple, etc.\n",
    "            b = x_batch.size(0)\n",
    "            total_samples += b\n",
    "\n",
    "            # empty dict if no conditions, else pass your dict\n",
    "            out_dict = {}\n",
    "\n",
    "            loss_multi, loss_gauss = random_mixed_loss(dm, x_batch, out_dict)\n",
    "\n",
    "            # loss_multi and loss_gauss are mean per batch, multiply by batch size for sum\n",
    "            total_loss_multi += loss_multi.item() * b\n",
    "            total_loss_gauss += loss_gauss.item() * b\n",
    "\n",
    "    return (total_loss_multi + total_loss_gauss) / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "55067fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM dataset loss: 0.7990\n",
      "DM dataset random loss: 2.9847\n"
     ]
    }
   ],
   "source": [
    "loss= evaluate_mixed_loss(diffusion, data_loader, device)\n",
    "print(f\"DM dataset loss: {loss:.4f}\")\n",
    "\n",
    "loss= evaluate_random_mixed_loss(diffusion, data_loader, device)\n",
    "print(f\"DM dataset random loss: {loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
