{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7a13c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f15cca6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>187.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>699.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>490.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>182.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     V1      V2    V3   V4     V5    V6     V7   V8   V9   V10 Class\n",
       "0  65.0  Female   0.7  0.1  187.0  16.0   18.0  6.8  3.3  0.90     1\n",
       "1  62.0    Male  10.9  5.5  699.0  64.0  100.0  7.5  3.2  0.74     1\n",
       "2  62.0    Male   7.3  4.1  490.0  60.0   68.0  7.0  3.3  0.89     1\n",
       "3  58.0    Male   1.0  0.4  182.0  14.0   20.0  6.8  3.4  1.00     1\n",
       "4  72.0    Male   3.9  2.0  195.0  27.0   59.0  7.3  2.4  0.40     1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'data/raw/phpOJxGL9.arff'\n",
    "\n",
    "data, meta = arff.loadarff(file_path)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7fcadbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[['V1', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "970579be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, categorical_cols=None, test_size=0.2):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Detect numeric and categorical\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_cols = df.columns.difference(categorical_cols).tolist()\n",
    "\n",
    "    # Encode categorical\n",
    "    encoder = OrdinalEncoder()\n",
    "    df[categorical_cols] = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "    # Scale numerical\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "    # Convert to torch\n",
    "    X = torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "    data_loader = DataLoader(TensorDataset(X), batch_size=256, shuffle=True)\n",
    "\n",
    "    return data_loader, len(numerical_cols), [int(df[col].nunique()) for col in categorical_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "68ab466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from src.tabddpm.modules import timestep_embedding, MLP\n",
    "\n",
    "class MLPDiffusion(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified MLP-based diffusion model for tabular data.\n",
    "\n",
    "    This model uses timestep embeddings and a projection layer\n",
    "    to inject time information into the input before processing it\n",
    "    through an MLP. It is intended for unsupervised tasks like clustering,\n",
    "    where label conditioning is not required.\n",
    "\n",
    "    Args:\n",
    "        d_in (int): Input feature dimension.\n",
    "        rtdl_params (dict): Parameters for the RTDL MLP (e.g., hidden sizes, depth).\n",
    "        dim_t (int): Dimensionality for the timestep embedding and projection space.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, rtdl_params, dim_t=128):\n",
    "        super().__init__()\n",
    "        self.dim_t = dim_t\n",
    "\n",
    "        # Configure MLP: input will be timestep-embedded, output must match original input size\n",
    "        rtdl_params['d_in'] = dim_t\n",
    "        rtdl_params['d_out'] = d_in\n",
    "        self.mlp = MLP.make_baseline(**rtdl_params)\n",
    "\n",
    "        # Project input features into timestep embedding space\n",
    "        self.proj = nn.Linear(d_in, dim_t)\n",
    "\n",
    "        # Timestep embedding network (2-layer MLP with SiLU activation)\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(dim_t, dim_t),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim_t, dim_t)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        \"\"\"\n",
    "        Forward pass of the MLPDiffusion model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, d_in).\n",
    "            timesteps (Tensor): Timestep tensor of shape (batch_size,) representing the diffusion step.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, d_in), same as the input dimension.\n",
    "        \"\"\"\n",
    "        # Get timestep embedding (e.g., sinusoidal), then pass through a small MLP\n",
    "        emb = self.time_embed(timestep_embedding(timesteps, self.dim_t))\n",
    "\n",
    "        # Project input to timestep embedding space and add time information\n",
    "        x = self.proj(x) + emb\n",
    "\n",
    "        # Pass through MLP and return\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1c7af97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tabddpm.gaussian_multinomial_diffusion import GaussianMultinomialDiffusion\n",
    "\n",
    "def get_diffusion_model(model, num_classes, num_numerical, device):\n",
    "    diffusion = GaussianMultinomialDiffusion(\n",
    "        num_classes=np.array(num_classes),\n",
    "        num_numerical_features=num_numerical,\n",
    "        denoise_fn=model,\n",
    "        gaussian_loss_type='mse',\n",
    "        num_timesteps=1000,\n",
    "        scheduler='cosine',\n",
    "        device=device\n",
    "    )\n",
    "    diffusion.to(device)\n",
    "    return diffusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dd29002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch.optim as optim\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, steps=1000, lr=1e-3, device='cuda'):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.steps = steps\n",
    "        self.device = device\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "        self.ema_model = deepcopy(model._denoise_fn)\n",
    "        for p in self.ema_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def train(self):\n",
    "        step = 0\n",
    "        iterator = iter(self.train_loader)\n",
    "\n",
    "        while step < self.steps:\n",
    "            try:\n",
    "                x_batch, = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(self.train_loader)\n",
    "                x_batch, = next(iterator)\n",
    "\n",
    "            x_batch = x_batch.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss_multi, loss_gauss = self.model.mixed_loss(x_batch, {})\n",
    "            loss = loss_multi + loss_gauss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"[{step}/{self.steps}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e1c24096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10000] Loss: 2.0804\n",
      "[100/10000] Loss: 1.3626\n",
      "[200/10000] Loss: 1.1430\n",
      "[300/10000] Loss: 0.9241\n",
      "[400/10000] Loss: 1.1916\n",
      "[500/10000] Loss: 1.1197\n",
      "[600/10000] Loss: 1.7262\n",
      "[700/10000] Loss: 1.1031\n",
      "[800/10000] Loss: 0.8292\n",
      "[900/10000] Loss: 1.0296\n",
      "[1000/10000] Loss: 0.8111\n",
      "[1100/10000] Loss: 0.6842\n",
      "[1200/10000] Loss: 0.9405\n",
      "[1300/10000] Loss: 0.9285\n",
      "[1400/10000] Loss: 0.6610\n",
      "[1500/10000] Loss: 2.2003\n",
      "[1600/10000] Loss: 1.2534\n",
      "[1700/10000] Loss: 0.5666\n",
      "[1800/10000] Loss: 0.8932\n",
      "[1900/10000] Loss: 0.7977\n",
      "[2000/10000] Loss: 1.3403\n",
      "[2100/10000] Loss: 0.6007\n",
      "[2200/10000] Loss: 0.7112\n",
      "[2300/10000] Loss: 0.8281\n",
      "[2400/10000] Loss: 0.8562\n",
      "[2500/10000] Loss: 0.7708\n",
      "[2600/10000] Loss: 0.6067\n",
      "[2700/10000] Loss: 0.9063\n",
      "[2800/10000] Loss: 0.9378\n",
      "[2900/10000] Loss: 0.6581\n",
      "[3000/10000] Loss: 0.7570\n",
      "[3100/10000] Loss: 0.9283\n",
      "[3200/10000] Loss: 0.9267\n",
      "[3300/10000] Loss: 0.8281\n",
      "[3400/10000] Loss: 1.2461\n",
      "[3500/10000] Loss: 0.6125\n",
      "[3600/10000] Loss: 0.7461\n",
      "[3700/10000] Loss: 0.9293\n",
      "[3800/10000] Loss: 0.8361\n",
      "[3900/10000] Loss: 0.7680\n",
      "[4000/10000] Loss: 1.0655\n",
      "[4100/10000] Loss: 0.7389\n",
      "[4200/10000] Loss: 0.8589\n",
      "[4300/10000] Loss: 0.8521\n",
      "[4400/10000] Loss: 1.4925\n",
      "[4500/10000] Loss: 0.6906\n",
      "[4600/10000] Loss: 0.7469\n",
      "[4700/10000] Loss: 1.3315\n",
      "[4800/10000] Loss: 0.6922\n",
      "[4900/10000] Loss: 0.8737\n",
      "[5000/10000] Loss: 0.8010\n",
      "[5100/10000] Loss: 0.9359\n",
      "[5200/10000] Loss: 0.9174\n",
      "[5300/10000] Loss: 1.4063\n",
      "[5400/10000] Loss: 0.9233\n",
      "[5500/10000] Loss: 0.7697\n",
      "[5600/10000] Loss: 0.8634\n",
      "[5700/10000] Loss: 0.8161\n",
      "[5800/10000] Loss: 0.8716\n",
      "[5900/10000] Loss: 0.5802\n",
      "[6000/10000] Loss: 1.1706\n",
      "[6100/10000] Loss: 0.8964\n",
      "[6200/10000] Loss: 0.6323\n",
      "[6300/10000] Loss: 0.8948\n",
      "[6400/10000] Loss: 0.8471\n",
      "[6500/10000] Loss: 0.7467\n",
      "[6600/10000] Loss: 0.7885\n",
      "[6700/10000] Loss: 0.9070\n",
      "[6800/10000] Loss: 0.6820\n",
      "[6900/10000] Loss: 1.0060\n",
      "[7000/10000] Loss: 1.1605\n",
      "[7100/10000] Loss: 1.0439\n",
      "[7200/10000] Loss: 1.2282\n",
      "[7300/10000] Loss: 1.0284\n",
      "[7400/10000] Loss: 0.7776\n",
      "[7500/10000] Loss: 0.6817\n",
      "[7600/10000] Loss: 0.7022\n",
      "[7700/10000] Loss: 0.6476\n",
      "[7800/10000] Loss: 0.8646\n",
      "[7900/10000] Loss: 1.0495\n",
      "[8000/10000] Loss: 1.0975\n",
      "[8100/10000] Loss: 1.0483\n",
      "[8200/10000] Loss: 0.9454\n",
      "[8300/10000] Loss: 0.6045\n",
      "[8400/10000] Loss: 1.2764\n",
      "[8500/10000] Loss: 0.6376\n",
      "[8600/10000] Loss: 1.3826\n",
      "[8700/10000] Loss: 1.0121\n",
      "[8800/10000] Loss: 0.8696\n",
      "[8900/10000] Loss: 0.5427\n",
      "[9000/10000] Loss: 0.7158\n",
      "[9100/10000] Loss: 0.8562\n",
      "[9200/10000] Loss: 0.9694\n",
      "[9300/10000] Loss: 1.1179\n",
      "[9400/10000] Loss: 0.7223\n",
      "[9500/10000] Loss: 0.8430\n",
      "[9600/10000] Loss: 0.7497\n",
      "[9700/10000] Loss: 0.8343\n",
      "[9800/10000] Loss: 1.0940\n",
      "[9900/10000] Loss: 1.0503\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_loader, num_numerical, num_classes = prepare_dataset(df_train, ['V2'])\n",
    "\n",
    "model_params = {\n",
    "    'num_classes': 0,\n",
    "    'is_y_cond': False,\n",
    "    'rtdl_params': {\n",
    "        'd_layers': [256, 256, 256],\n",
    "        'dropout': 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "mlp_model = MLPDiffusion(\n",
    "    d_in=num_numerical + sum(num_classes),\n",
    "    rtdl_params=model_params['rtdl_params']\n",
    ")\n",
    "\n",
    "mlp_model.to(device)\n",
    "\n",
    "diffusion = get_diffusion_model(mlp_model, num_classes, num_numerical, device)\n",
    "\n",
    "trainer = Trainer(diffusion, data_loader, steps=10000, device=device, lr=0.001)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db8292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2012, -0.3061, -0.3158,  ..., -1.1850, -1.0592,  1.0000],\n",
      "        [ 0.9430,  1.2413,  1.2166,  ..., -1.3108, -1.0906,  1.0000],\n",
      "        [-1.7153, -0.4511, -0.4940,  ...,  1.7087,  2.3661,  0.0000],\n",
      "        ...,\n",
      "        [-1.1589, -0.4350, -0.4583,  ..., -0.9333, -0.6193,  0.0000],\n",
      "        [-0.7880,  3.1271,  3.1054,  ..., -0.4301, -0.7764,  1.0000],\n",
      "        [-0.2934, -0.4189, -0.4940,  ..., -0.6817,  0.1664,  1.0000]])\n",
      "tensor([[ 0.0989,  0.5668, -0.5592,  ..., -0.7411, -0.5338,  0.5286],\n",
      "        [ 0.0085,  0.2609, -0.0081,  ...,  0.3516, -1.7337,  1.1427],\n",
      "        [-0.1041, -0.1533,  0.2487,  ...,  0.0658, -0.6076,  0.5510],\n",
      "        ...,\n",
      "        [-1.0366, -0.3861,  0.4864,  ..., -0.6828, -0.6099,  0.3700],\n",
      "        [ 0.1357,  0.5098, -0.1787,  ..., -0.4231, -1.6590,  2.2879],\n",
      "        [ 0.1465, -1.6179,  0.8162,  ..., -0.4069, -0.4884,  0.5847]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def index_to_log_onehot(x, num_classes):\n",
    "    onehots = []\n",
    "    for i in range(len(num_classes)):\n",
    "        onehots.append(F.one_hot(x[:, i], num_classes[i]))\n",
    " \n",
    "    x_onehot = torch.cat(onehots, dim=1)\n",
    "    log_onehot = torch.log(x_onehot.float().clamp(min=1e-30))\n",
    "    return log_onehot\n",
    "\n",
    "x = next(iter(data_loader))[0]\n",
    "b = x.shape[0]\n",
    "t, pt = diffusion.sample_time(b, device, 'uniform')\n",
    "\n",
    "x_num = x[:, :diffusion.num_numerical_features]\n",
    "x_cat = x[:, diffusion.num_numerical_features:]\n",
    "\n",
    "x_num_t = x_num\n",
    "log_x_cat_t = x_cat\n",
    "if x_num.shape[1] > 0:\n",
    "    noise = torch.randn_like(x_num)\n",
    "    x_num_t = diffusion.gaussian_q_sample(x_num, t, noise=noise)\n",
    "if x_cat.shape[1] > 0:\n",
    "    log_x_cat = index_to_log_onehot(x_cat.long(), diffusion.num_classes)\n",
    "    log_x_cat_t = diffusion.q_sample(log_x_start=log_x_cat, t=t)\n",
    "\n",
    "x_in = torch.cat([x_num_t, log_x_cat_t], dim=1)\n",
    "\n",
    "model_out = diffusion._denoise_fn(\n",
    "    x_in,\n",
    "    t\n",
    ")\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(model_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10416f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5338,  0.5286],\n",
       "        [-1.7337,  1.1427],\n",
       "        [-0.6076,  0.5510],\n",
       "        [-1.4578,  1.3563],\n",
       "        [-0.6501,  0.7371],\n",
       "        [-1.1912,  1.1144],\n",
       "        [-0.6940,  0.4939],\n",
       "        [-0.5740,  0.7254],\n",
       "        [-0.3510,  0.9638],\n",
       "        [-0.6088,  0.9492],\n",
       "        [ 2.9420, -2.4517],\n",
       "        [-0.6559,  0.6354],\n",
       "        [-0.2866,  0.2872],\n",
       "        [-1.2745,  1.1057],\n",
       "        [-0.5235,  0.6040],\n",
       "        [-0.1400,  1.1176],\n",
       "        [ 0.5576, -0.4885],\n",
       "        [-3.2497,  4.2588],\n",
       "        [-0.3476,  0.4398],\n",
       "        [-0.8132,  0.3300],\n",
       "        [-0.4699,  0.7250],\n",
       "        [-0.1434,  0.8091],\n",
       "        [-0.7831,  0.4507],\n",
       "        [-0.4659,  0.0859],\n",
       "        [-0.4265,  0.4140],\n",
       "        [-0.3229,  0.5318],\n",
       "        [-0.8175,  0.9610],\n",
       "        [-0.2345,  0.5297],\n",
       "        [-0.5385,  0.5333],\n",
       "        [-0.6286,  0.7021],\n",
       "        [-0.8803,  0.6137],\n",
       "        [-0.4869,  0.1292],\n",
       "        [-0.2821,  0.3958],\n",
       "        [-0.6323,  0.7234],\n",
       "        [-0.2907,  0.3417],\n",
       "        [-0.4034,  0.4679],\n",
       "        [-0.6100,  0.7046],\n",
       "        [-0.0173,  0.2178],\n",
       "        [-0.5610,  0.3135],\n",
       "        [-0.8402,  0.4910],\n",
       "        [-0.1757,  0.5863],\n",
       "        [-1.1793,  1.2724],\n",
       "        [-0.1994,  0.3770],\n",
       "        [-1.0344,  0.2834],\n",
       "        [-0.4508,  0.4896],\n",
       "        [-0.4196,  0.5147],\n",
       "        [-0.6565,  0.6100],\n",
       "        [-1.0676,  0.8293],\n",
       "        [-0.5859,  1.1366],\n",
       "        [-0.4841,  0.6058],\n",
       "        [-1.0249,  0.5013],\n",
       "        [-0.9559,  0.6813],\n",
       "        [-1.0099,  1.2635],\n",
       "        [-0.0693,  0.2854],\n",
       "        [-1.0531,  1.1577],\n",
       "        [-0.4776,  0.6623],\n",
       "        [-0.1257,  0.5347],\n",
       "        [-0.7400,  0.3209],\n",
       "        [-0.2999,  0.3488],\n",
       "        [-0.5007,  0.7069],\n",
       "        [-0.4745,  0.7168],\n",
       "        [-2.0648,  2.7370],\n",
       "        [-0.8060,  0.2658],\n",
       "        [-0.6067,  0.5132],\n",
       "        [-0.5826,  0.5945],\n",
       "        [-0.6413,  0.6278],\n",
       "        [-1.3057,  1.2093],\n",
       "        [-0.6379,  0.7530],\n",
       "        [-0.9043,  0.6629],\n",
       "        [-0.1444,  0.4643],\n",
       "        [-0.5814,  0.8806],\n",
       "        [-0.4426,  0.4265],\n",
       "        [-0.3607,  0.5207],\n",
       "        [-1.1559,  0.8193],\n",
       "        [-0.6728,  0.5759],\n",
       "        [-0.6039,  0.6520],\n",
       "        [-0.6394,  0.6004],\n",
       "        [-0.4338,  0.4574],\n",
       "        [-0.8351,  0.6006],\n",
       "        [-1.0008,  0.8082],\n",
       "        [-0.7602,  0.9346],\n",
       "        [-0.8490,  0.5459],\n",
       "        [-0.7162,  0.4559],\n",
       "        [-0.7546,  0.7867],\n",
       "        [-0.6250,  0.6014],\n",
       "        [-1.2299,  1.2409],\n",
       "        [-0.9421,  0.8198],\n",
       "        [-0.4954,  0.7952],\n",
       "        [-0.1418,  0.4462],\n",
       "        [-0.2739,  0.3267],\n",
       "        [-0.6850,  0.5443],\n",
       "        [-0.4130,  0.5280],\n",
       "        [-0.4133,  0.3903],\n",
       "        [-0.5247,  0.4707],\n",
       "        [-0.0758,  0.4390],\n",
       "        [-1.9998,  2.5045],\n",
       "        [-0.4458,  0.8528],\n",
       "        [-1.0456,  0.4393],\n",
       "        [-0.8170,  0.9015],\n",
       "        [-0.4423,  0.4406],\n",
       "        [-0.3117,  0.9615],\n",
       "        [-0.4539,  0.4981],\n",
       "        [-0.4066,  0.2584],\n",
       "        [-0.2334,  0.4472],\n",
       "        [-0.6683,  0.7099],\n",
       "        [-0.0654,  0.6657],\n",
       "        [-0.4043,  0.6557],\n",
       "        [-0.5542,  0.3710],\n",
       "        [-1.4508,  1.8757],\n",
       "        [-1.6937,  2.3541],\n",
       "        [-0.8325,  0.7785],\n",
       "        [ 0.1684,  0.1274],\n",
       "        [-0.3099,  0.3090],\n",
       "        [-0.3456,  0.8019],\n",
       "        [-1.1864,  0.9087],\n",
       "        [-0.7575,  0.8131],\n",
       "        [-0.8569,  0.8152],\n",
       "        [-0.5766,  0.6006],\n",
       "        [-0.8638,  0.7673],\n",
       "        [-0.8166,  0.9522],\n",
       "        [-0.4678,  0.8579],\n",
       "        [-0.3602,  0.4777],\n",
       "        [ 0.0552,  0.2685],\n",
       "        [-0.4685,  0.5794],\n",
       "        [-0.6608,  0.5672],\n",
       "        [-3.2228,  4.6325],\n",
       "        [-0.9763,  0.7112],\n",
       "        [-0.7219,  0.8143],\n",
       "        [-0.3589,  0.3753],\n",
       "        [-0.8207,  1.3086],\n",
       "        [-0.3122,  0.5109],\n",
       "        [-0.7175,  0.4827],\n",
       "        [-0.6261,  0.7531],\n",
       "        [-0.3117,  0.0893],\n",
       "        [-0.3930,  0.7006],\n",
       "        [-0.4657,  0.5047],\n",
       "        [-0.3466,  0.6610],\n",
       "        [-0.7027,  0.7419],\n",
       "        [ 0.0149,  0.0671],\n",
       "        [-0.5576,  0.2445],\n",
       "        [-0.0822,  0.6544],\n",
       "        [-0.1980,  0.4523],\n",
       "        [-0.4850,  0.6899],\n",
       "        [-0.1657,  0.5169],\n",
       "        [-0.1459,  0.4556],\n",
       "        [-0.7437,  0.5449],\n",
       "        [-0.5175,  0.6312],\n",
       "        [-0.3886,  0.5020],\n",
       "        [-0.0679,  0.6283],\n",
       "        [-0.5099,  0.6155],\n",
       "        [-0.5569,  0.7551],\n",
       "        [-1.2211,  1.3401],\n",
       "        [-0.3846,  0.7608],\n",
       "        [-0.5598,  0.9258],\n",
       "        [-0.8859,  0.9832],\n",
       "        [-0.5272,  0.6715],\n",
       "        [-0.6199,  0.5477],\n",
       "        [-0.5732,  0.6353],\n",
       "        [-0.5887,  0.5588],\n",
       "        [-0.2502,  0.5229],\n",
       "        [-0.8965,  1.1088],\n",
       "        [-0.6137,  0.7782],\n",
       "        [-0.1281,  0.7400],\n",
       "        [-0.2933,  0.6452],\n",
       "        [-0.3457,  0.4160],\n",
       "        [-0.5573,  0.7376],\n",
       "        [-0.1759,  0.4086],\n",
       "        [-0.5136,  0.8266],\n",
       "        [-0.6308,  0.6985],\n",
       "        [-0.4375,  0.3945],\n",
       "        [-0.5301,  0.6564],\n",
       "        [-0.4209,  0.7342],\n",
       "        [-0.3276,  0.8544],\n",
       "        [-0.0632,  0.0161],\n",
       "        [ 0.0456,  0.4914],\n",
       "        [-0.3162,  0.3308],\n",
       "        [ 6.4689, -5.6582],\n",
       "        [-0.4057,  0.4274],\n",
       "        [-0.4741,  0.4698],\n",
       "        [-0.4496,  0.7005],\n",
       "        [-0.3300,  0.6524],\n",
       "        [-0.1081,  0.2634],\n",
       "        [-0.5763,  0.5821],\n",
       "        [-0.9567,  0.4217],\n",
       "        [-0.8182,  1.0482],\n",
       "        [-0.4901,  0.6978],\n",
       "        [-1.5083,  1.4817],\n",
       "        [-1.2145,  1.4601],\n",
       "        [-0.6059,  0.8735],\n",
       "        [-1.2477,  2.0175],\n",
       "        [-0.2626,  0.7195],\n",
       "        [-0.8500,  0.6491],\n",
       "        [-0.7121,  1.0401],\n",
       "        [-0.2012,  0.3583],\n",
       "        [ 0.0467,  0.8388],\n",
       "        [-0.3912,  0.6443],\n",
       "        [-0.6254,  1.0704],\n",
       "        [-0.6379,  0.6302],\n",
       "        [-0.5685,  0.4807],\n",
       "        [-0.4101,  0.5567],\n",
       "        [-0.4670,  0.5827],\n",
       "        [-1.3635,  1.0141],\n",
       "        [-0.8293,  0.5210],\n",
       "        [-0.3157,  0.8458],\n",
       "        [-0.1627,  0.7622],\n",
       "        [-0.3021,  0.7478],\n",
       "        [-0.9867,  0.8836],\n",
       "        [-1.1514,  1.0330],\n",
       "        [-1.3932,  1.7940],\n",
       "        [-0.9968,  0.8785],\n",
       "        [-0.2360,  0.3071],\n",
       "        [-0.2806,  0.6973],\n",
       "        [-0.1270,  0.6210],\n",
       "        [-3.3114,  3.9465],\n",
       "        [-0.2355,  0.2914],\n",
       "        [-0.2099,  0.3960],\n",
       "        [-0.6438,  0.7810],\n",
       "        [ 0.8310, -0.9609],\n",
       "        [-0.3262,  0.5725],\n",
       "        [-0.8113,  0.5458],\n",
       "        [-0.3503,  0.5827],\n",
       "        [-0.3532,  0.5796],\n",
       "        [-0.5375,  0.4294],\n",
       "        [-0.8690,  0.6885],\n",
       "        [-0.7544,  0.6394],\n",
       "        [-0.4328,  0.5503],\n",
       "        [-0.4671,  0.5182],\n",
       "        [-0.8825,  0.5630],\n",
       "        [-0.5465,  0.0543],\n",
       "        [-0.2862,  0.7425],\n",
       "        [-0.3410,  0.9123],\n",
       "        [-0.9609,  0.3564],\n",
       "        [-0.5811,  0.5522],\n",
       "        [-0.4945,  0.3868],\n",
       "        [-0.3971,  0.6830],\n",
       "        [-0.6664,  0.4720],\n",
       "        [-0.1575,  0.6993],\n",
       "        [-0.3708,  0.2517],\n",
       "        [-0.4864,  0.4269],\n",
       "        [-0.1978,  0.4499],\n",
       "        [-0.6084,  0.7277],\n",
       "        [-0.5932,  0.8805],\n",
       "        [-0.5978,  0.6534],\n",
       "        [-0.9687,  0.8980],\n",
       "        [-0.1153,  0.3900],\n",
       "        [-0.9304,  1.0960],\n",
       "        [-0.0639,  0.4806],\n",
       "        [-1.1350,  1.0248],\n",
       "        [-0.0424,  0.7575],\n",
       "        [-0.6430,  0.7052],\n",
       "        [-1.3095,  1.0721],\n",
       "        [-0.4931,  0.3899],\n",
       "        [-1.6723,  1.5297],\n",
       "        [-0.6099,  0.3700],\n",
       "        [-1.6590,  2.2879],\n",
       "        [-0.4884,  0.5847]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out_cat = model_out[:, diffusion.num_numerical_features:]\n",
    "model_out_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b79d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mixed_loss(dm, data_loader, device):\n",
    "    dm.eval()\n",
    "    total_loss_multi = 0.0\n",
    "    total_loss_gauss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, in data_loader:\n",
    "            x_batch = batch.to(device).float()  # adjust if your loader returns tuple, etc.\n",
    "            b = x_batch.size(0)\n",
    "            total_samples += b\n",
    "\n",
    "            # empty dict if no conditions, else pass your dict\n",
    "            out_dict = {}\n",
    "\n",
    "            loss_multi, loss_gauss = dm.mixed_loss(x_batch, out_dict)\n",
    "\n",
    "            # loss_multi and loss_gauss are mean per batch, multiply by batch size for sum\n",
    "            total_loss_multi += loss_multi.item() * b\n",
    "            total_loss_gauss += loss_gauss.item() * b\n",
    "\n",
    "    return (total_loss_multi + total_loss_gauss) / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a5e31109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mixed_loss(dm, x, out_dict):\n",
    "    b = x.shape[0]\n",
    "    device = x.device\n",
    "    t, pt = dm.sample_time(b, device, 'uniform')\n",
    "\n",
    "    x_num = x[:, :dm.num_numerical_features]\n",
    "    x_cat = x[:, dm.num_numerical_features:]\n",
    "    \n",
    "    x_num_t = x_num\n",
    "    log_x_cat_t = x_cat\n",
    "    if x_num.shape[1] > 0:\n",
    "        noise = torch.randn_like(x_num)\n",
    "        x_num_t = dm.gaussian_q_sample(x_num, t, noise=noise)\n",
    "    if x_cat.shape[1] > 0:\n",
    "        log_x_cat = index_to_log_onehot(x_cat.long(), dm.num_classes)\n",
    "        log_x_cat_t = dm.q_sample(log_x_start=log_x_cat, t=t)\n",
    "    \n",
    "    x_in = torch.cat([x_num_t, log_x_cat_t], dim=1)\n",
    "\n",
    "    model_out = torch.randn_like(x_in)\n",
    "\n",
    "    model_out_num = model_out[:, :dm.num_numerical_features]\n",
    "    model_out_cat = model_out[:, dm.num_numerical_features:]\n",
    "\n",
    "    loss_multi = torch.zeros((1,)).float()\n",
    "    loss_gauss = torch.zeros((1,)).float()\n",
    "    if x_cat.shape[1] > 0:\n",
    "        loss_multi = dm._multinomial_loss(model_out_cat, log_x_cat, log_x_cat_t, t, pt, out_dict) / len(dm.num_classes)\n",
    "    \n",
    "    if x_num.shape[1] > 0:\n",
    "        loss_gauss = dm._gaussian_loss(model_out_num, x_num, x_num_t, t, noise)\n",
    "\n",
    "    # loss_multi = torch.where(out_dict['y'] == 1, loss_multi, 2 * loss_multi)\n",
    "    # loss_gauss = torch.where(out_dict['y'] == 1, loss_gauss, 2 * loss_gauss)\n",
    "\n",
    "    return loss_multi.mean(), loss_gauss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ab307d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random_mixed_loss(dm, data_loader, device):\n",
    "    dm.eval()\n",
    "    total_loss_multi = 0.0\n",
    "    total_loss_gauss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, in data_loader:\n",
    "            x_batch = batch.to(device).float()  # adjust if your loader returns tuple, etc.\n",
    "            b = x_batch.size(0)\n",
    "            total_samples += b\n",
    "\n",
    "            # empty dict if no conditions, else pass your dict\n",
    "            out_dict = {}\n",
    "\n",
    "            loss_multi, loss_gauss = random_mixed_loss(dm, x_batch, out_dict)\n",
    "\n",
    "            # loss_multi and loss_gauss are mean per batch, multiply by batch size for sum\n",
    "            total_loss_multi += loss_multi.item() * b\n",
    "            total_loss_gauss += loss_gauss.item() * b\n",
    "\n",
    "    return (total_loss_multi + total_loss_gauss) / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "55067fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM dataset loss: 0.8420\n",
      "DM dataset random loss: 2.8375\n"
     ]
    }
   ],
   "source": [
    "loss= evaluate_mixed_loss(diffusion, data_loader, device)\n",
    "print(f\"DM dataset loss: {loss:.4f}\")\n",
    "\n",
    "loss= evaluate_random_mixed_loss(diffusion, data_loader, device)\n",
    "print(f\"DM dataset random loss: {loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
