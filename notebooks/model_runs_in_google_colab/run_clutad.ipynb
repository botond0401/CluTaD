{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiYugevDYQWk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TabularEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder for tabular data.\n",
        "    Maps input features to latent mean and log-variance for Gaussian latent space.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims, latent_dim):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, h_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_dim = h_dim\n",
        "        self.feature_extractor = nn.Sequential(*layers)\n",
        "\n",
        "        self.mu_layer = nn.Linear(prev_dim, latent_dim)\n",
        "        self.logvar_layer = nn.Linear(prev_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, input_dim)\n",
        "\n",
        "        Returns:\n",
        "            mu: (batch_size, latent_dim)\n",
        "            logvar: (batch_size, latent_dim)\n",
        "        \"\"\"\n",
        "        h = self.feature_extractor(x)\n",
        "        mu = self.mu_layer(h)\n",
        "        logvar = self.logvar_layer(h)\n",
        "        return mu, logvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS2EeGJwYYIS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Denoiser(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple denoiser model for tabular diffusion.\n",
        "    Applies a linear -> ReLU -> linear architecture.\n",
        "    Outputs numerical predictions and categorical probabilities (via softmax).\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_in, latent_dim, dim_hidden, num_numeric, categories):\n",
        "        super().__init__()\n",
        "        self.num_numeric = num_numeric\n",
        "        self.categories = categories\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim_in + latent_dim + 1, dim_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_hidden, dim_in)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, z, t):\n",
        "        \"\"\"\n",
        "        Forward pass of the denoiser.\n",
        "\n",
        "        Args:\n",
        "            x: Tensor, shape (batch_size, dim_in)\n",
        "            t: Tensor, shape (batch_size,)\n",
        "\n",
        "        Returns:\n",
        "            out_num: numerical denoised output\n",
        "            out_cat: categorical probabilities after softmax\n",
        "        \"\"\"\n",
        "        t = t.unsqueeze(1).float()\n",
        "        xzt = torch.cat([x, z, t], dim=1)\n",
        "        out = self.net(xzt)\n",
        "\n",
        "        out_num = out[:, :self.num_numeric]\n",
        "        out_cat_raw = out[:, self.num_numeric:]\n",
        "\n",
        "        out_cat = []\n",
        "        idx = 0\n",
        "        for K in self.categories:\n",
        "            logits = out_cat_raw[:, idx:idx+K]\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            out_cat.append(probs)\n",
        "            idx += K\n",
        "\n",
        "        out_cat = torch.cat(out_cat, dim=1) if out_cat else None\n",
        "        return out_num, out_cat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwXSeIdKYYiv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import torch.nn.utils as nn_utils\n",
        "import itertools\n",
        "\n",
        "class CluTaD:\n",
        "    \"\"\"\n",
        "    CluTaD model: wraps encoder, denoiser, diffusion schedules, and GMM.\n",
        "    Supports pretraining, ELBO training, GMM fitting, and sampling.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, denoiser, T, num_numeric, categories, n_clusters, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.denoiser = denoiser\n",
        "        self.T = T\n",
        "        self.num_numeric = num_numeric\n",
        "        self.categories = categories\n",
        "        self.n_clusters = n_clusters\n",
        "        self.device = device\n",
        "\n",
        "        # Diffusion schedules\n",
        "        betas = 0.01 * torch.arange(1, T + 1).float() / T\n",
        "        alphas = 1 - betas\n",
        "        self.alpha_bars = torch.cumprod(alphas, dim=0).to(device)\n",
        "        self.sqrtab = self.alpha_bars.sqrt()\n",
        "        self.sqrtmab = (1 - self.alpha_bars).sqrt()\n",
        "\n",
        "        self.gmm = None  # Will hold fitted GMM\n",
        "\n",
        "        # MLP head for auxiliary distribution Q\n",
        "        self.mlp = nn.Linear(encoder.latent_dim, n_clusters).to(device)\n",
        "\n",
        "        # Priors (π): uniform if not provided\n",
        "        #if pi is None:\n",
        "         #   self.pi = torch.full((n_clusters,), 1.0 / n_clusters, device=device)\n",
        "        #else:\n",
        "         #   self.pi = torch.tensor(pi, dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "    def pretrain_step(self, x, optimizer):\n",
        "        \"\"\"\n",
        "        One pretraining step: predict noise from x_t + z + t\n",
        "        \"\"\"\n",
        "        B = x.shape[0]\n",
        "        t = torch.randint(1, self.T + 1, (B,), device=self.device) - 1\n",
        "        noise = torch.randn_like(x)\n",
        "\n",
        "        x_t = self.sqrtab[t].unsqueeze(1) * x + self.sqrtmab[t].unsqueeze(1) * noise\n",
        "\n",
        "        mu, logvar = self.encoder(x)\n",
        "        z = mu + torch.randn_like(mu) * (0.5 * logvar).exp()\n",
        "\n",
        "        t_norm = t.float() / self.T\n",
        "        pred_num, pred_cat = self.denoiser(x_t, z, t_norm)\n",
        "\n",
        "        if self.num_numeric > 0 and pred_num is not None:\n",
        "            noise_num = noise[:, :self.num_numeric]\n",
        "            loss_num = ((pred_num - noise_num) ** 2).mean()\n",
        "        else:\n",
        "            loss_num = torch.zeros((), device=x.device)\n",
        "\n",
        "        loss_cat = torch.zeros((), device=x.device)\n",
        "\n",
        "        has_cats = bool(self.categories) and sum(self.categories) > 0\n",
        "        if has_cats and pred_cat is not None:\n",
        "          x0_cat = x[:, self.num_numeric:]\n",
        "          idx_c = 0\n",
        "          for K in self.categories:\n",
        "              target = x0_cat[:, idx_c:idx_c+K]\n",
        "              pred_prob = pred_cat[:, idx_c:idx_c+K]\n",
        "              kl = (target * (torch.log(target + 1e-10) - torch.log(pred_prob + 1e-10))).sum(1).mean()\n",
        "              loss_cat += kl\n",
        "              idx_c += K\n",
        "          if len(self.categories) > 0:\n",
        "              loss_cat /= len(self.categories)\n",
        "\n",
        "        loss = loss_num + loss_cat\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn_utils.clip_grad_norm_(\n",
        "            itertools.chain(self.encoder.parameters(), self.denoiser.parameters()),\n",
        "            max_norm=0.5\n",
        "        )\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss.item(), loss_num.item(), loss_cat.item()\n",
        "\n",
        "\n",
        "    def pretrain(self, dataloader, optimizer, epochs, batch_size, plot_freq=100):\n",
        "        \"\"\"\n",
        "        Pretrain encoder + denoiser over multiple steps.\n",
        "\n",
        "        Args:\n",
        "            dataloader: full dataset tensor (N, D)\n",
        "            optimizer: optimizer for encoder + denoiser\n",
        "            epochs: number of pretraining epochs\n",
        "            batch_size: batch size\n",
        "            plot_freq: print loss every plot_freq steps\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            n_samples = 0\n",
        "            for (x_batch,) in dataloader:\n",
        "                if x_batch.ndim == 1:\n",
        "                    x_batch = x_batch.unsqueeze(0)\n",
        "                x_batch = x_batch.to(self.device)\n",
        "\n",
        "                loss, loss_num, loss_cat = self.pretrain_step(x_batch, optimizer)\n",
        "\n",
        "                total_loss += loss * x_batch.size(0)\n",
        "                n_samples += x_batch.size(0)\n",
        "\n",
        "            avg_loss = total_loss / n_samples\n",
        "            if (epoch+1) % plot_freq == 0:\n",
        "              print(f'[Pretrain] Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "\n",
        "    def fit_gmm(self, dataloader):\n",
        "        \"\"\"\n",
        "        Fit a Gaussian Mixture Model on the latent space.\n",
        "\n",
        "        Args:\n",
        "            x_real: full dataset tensor (N, D)\n",
        "            n_clusters: number of clusters to fit\n",
        "        \"\"\"\n",
        "        self.encoder.eval()\n",
        "        latent_z = []\n",
        "        with torch.no_grad():\n",
        "            for (x,) in dataloader:\n",
        "                x = x.to(self.device)\n",
        "                z_mu, z_sigma2_log = self.encoder(x)\n",
        "                z = torch.randn_like(z_mu) * torch.exp(z_sigma2_log / 2) + z_mu\n",
        "                latent_z.append(z)\n",
        "        latent_z = torch.cat(latent_z, 0).detach().cpu().numpy()\n",
        "\n",
        "        if self.gmm is not None:\n",
        "            init_means = self.gmm.means_\n",
        "            init_precisions = self.gmm.precisions_\n",
        "            gmm = GaussianMixture(n_components = self.n_clusters,\n",
        "                                  covariance_type = 'diag',\n",
        "                                  reg_covar=1e-1,\n",
        "                                  means_init = init_means,\n",
        "                                  precisions_init = init_precisions)\n",
        "        else:\n",
        "          gmm = GaussianMixture(n_components=self.n_clusters, covariance_type='diag', reg_covar=1e-2)\n",
        "        gmm.fit(latent_z)\n",
        "        #gmm.weights_ = self.pi.detach().cpu().numpy()\n",
        "        self.gmm = gmm\n",
        "        #print(f\"✅ GMM fitted with {self.n_clusters} components\")\n",
        "\n",
        "\n",
        "    def elbo_step(self, x, optimizer, kl_weight=0.1):\n",
        "        \"\"\"\n",
        "        One ELBO training step for CluTaD.\n",
        "\n",
        "        Args:\n",
        "            x: input batch (B, D)\n",
        "            optimizer: optimizer\n",
        "            kl_weight: weight for the KL terms\n",
        "\n",
        "        Returns:\n",
        "            total_loss, rec_loss, kl_loss\n",
        "        \"\"\"\n",
        "        B = x.shape[0]\n",
        "        t = torch.randint(1, self.T + 1, (B,), device=self.device) - 1\n",
        "        noise = torch.randn_like(x)\n",
        "\n",
        "        # Diffusion forward process\n",
        "        x_t = self.sqrtab[t].unsqueeze(1) * x + self.sqrtmab[t].unsqueeze(1) * noise\n",
        "\n",
        "        # Encoder\n",
        "        mu_phi, logvar_phi = self.encoder(x)\n",
        "        sigma2_phi = torch.exp(logvar_phi)\n",
        "        z = mu_phi + torch.randn_like(mu_phi) * (0.5 * logvar_phi).exp()\n",
        "\n",
        "        # Denoising\n",
        "        t_norm = t.float() / self.T\n",
        "        pred_num, pred_cat = self.denoiser(x_t, z, t_norm)\n",
        "\n",
        "        # Reconstruction loss\n",
        "        if self.num_numeric > 0 and pred_num is not None:\n",
        "            noise_num = noise[:, :self.num_numeric]\n",
        "            rec_loss_num = ((pred_num - noise_num) ** 2).mean()\n",
        "        else:\n",
        "            rec_loss_num = torch.zeros((), device=x.device)\n",
        "\n",
        "        rec_loss_cat = 0.0\n",
        "        x0_cat = x[:, self.num_numeric:]\n",
        "        idx_c = 0\n",
        "        for K in self.categories:\n",
        "            target = x0_cat[:, idx_c:idx_c + K]\n",
        "            pred_prob = pred_cat[:, idx_c:idx_c + K]\n",
        "            kl = (target * (torch.log(target + 1e-10) - torch.log(pred_prob + 1e-10))).sum(1).mean()\n",
        "            rec_loss_cat += kl\n",
        "            idx_c += K\n",
        "        if len(self.categories) > 0:\n",
        "            rec_loss_cat /= len(self.categories)\n",
        "\n",
        "        rec_loss = rec_loss_num + rec_loss_cat\n",
        "\n",
        "        # ===== Cluster assignments =====\n",
        "        # Q from MLP head\n",
        "        logits = self.mlp(z)        # (B, K)\n",
        "        Q = F.softmax(logits, dim=1)\n",
        "\n",
        "        # P from GMM (Mahalanobis distance + softmax)\n",
        "        B, D = z.shape\n",
        "        z_exp = z.unsqueeze(1).expand(B, self.n_clusters, D)                   # (B, K, D)\n",
        "        mu = torch.from_numpy(self.gmm.means_).to(z.device).float().unsqueeze(0)      # (1, K, D)\n",
        "        var = torch.from_numpy(self.gmm.covariances_).to(z.device).float().unsqueeze(0)  # (1, K, D)\n",
        "\n",
        "        dist = torch.sqrt(((z_exp - mu) ** 2 / var).sum(dim=2)) /2             # (B, K)\n",
        "        P = F.softmax(-dist, dim=1)                                            # (B, K)\n",
        "\n",
        "        # Update priors for stopping criterion\n",
        "        #self.pi = P.mean(dim=0).detach()\n",
        "\n",
        "        # ===== Clustering loss KL(P||Q) =====\n",
        "        # cluster_loss = F.kl_div(Q.log(), P, reduction='batchmean')\n",
        "        cluster_loss = -(Q * torch.log(P + 1e-5)).sum(dim=1).mean()\n",
        "\n",
        "        ()\n",
        "\n",
        "        # ===== Total loss =====\n",
        "        total_loss = rec_loss + kl_weight * cluster_loss\n",
        "\n",
        "        # Backward + update\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        nn_utils.clip_grad_norm_(\n",
        "            itertools.chain(self.encoder.parameters(),\n",
        "                            self.denoiser.parameters(),\n",
        "                            self.mlp.parameters()),\n",
        "            max_norm=0.5\n",
        "        )\n",
        "        optimizer.step()\n",
        "\n",
        "        return total_loss.item(), rec_loss.item(), cluster_loss.item()\n",
        "\n",
        "\n",
        "    def train_elbo(self, dataloader, optimizer, batch_size, kl_weight=0.1, plot_freq=100):\n",
        "        \"\"\"\n",
        "        ELBO training loop: combines reconstruction + KL loss.\n",
        "\n",
        "        Args:\n",
        "            x_real: dataset (N, D)\n",
        "            optimizer: optimizer\n",
        "            batch_size: batch size\n",
        "            kl_weight: weight on KL\n",
        "            plot_freq: print every plot_freq steps\n",
        "        \"\"\"\n",
        "        self.encoder.train()\n",
        "        self.denoiser.train()\n",
        "        self.mlp.train()\n",
        "        total_loss = 0.0\n",
        "        total_recon_loss = 0.0\n",
        "        total_kl_loss = 0.0\n",
        "        n_samples = 0\n",
        "\n",
        "        for (x_batch,) in dataloader:\n",
        "            if x_batch.ndim == 1:\n",
        "                x_batch = x_batch.unsqueeze(0)\n",
        "            x_batch = x_batch.to(self.device)\n",
        "            loss, rec_loss, kl_loss = self.elbo_step(x_batch, optimizer, kl_weight=kl_weight)\n",
        "\n",
        "            total_loss += loss * x_batch.size(0)\n",
        "            total_recon_loss += rec_loss * x_batch.size(0)\n",
        "            total_kl_loss += kl_loss * x_batch.size(0)\n",
        "            n_samples += x_batch.size(0)\n",
        "\n",
        "        avg_loss = total_loss / n_samples\n",
        "        avg_recon_loss = total_recon_loss / n_samples\n",
        "        avg_kl_loss = total_kl_loss / n_samples\n",
        "\n",
        "        # ===== Early stopping check =====\n",
        "        stop = False\n",
        "        if (np.any(self.gmm.weights_ <= 1.0 / (2 * self.n_clusters))):\n",
        "        #if (self.n_clusters > 2) and (np.any(self.gmm.weights_ <= 1.0 / (2 * self.n_clusters))):\n",
        "            #print(f\"[Early Stopping] Cluster prior too small: {self.gmm.weights_}\")\n",
        "            stop = True\n",
        "\n",
        "        return avg_loss, avg_recon_loss, avg_kl_loss, stop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB3jQxzWYYww",
        "outputId": "bd8d4901-24e4-477c-cc50-5f53e62f597a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dim_hidden 500, latent_dim: 5\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.2738\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.2489\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.2063\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.1634\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.1325\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.1021\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.0829\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.0876\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.0833\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.1009\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "⏹️ Stopping early at epoch 1\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.6611\n",
            "ARI: 0.4034\n",
            "Saved results to clutad_PCA_results.json\n",
            "\n",
            "💾 New best model saved to best_checkpoint_40982l.pth (acc=0.6611, T=100, z=5)\n",
            "dim_hidden 500, latent_dim: 10\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.2690\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.2702\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.1882\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.1508\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.1212\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.1030\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.0896\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.0933\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.0975\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.0926\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "⏹️ Stopping early at epoch 32\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.7848\n",
            "ARI: 0.7178\n",
            "Saved results to clutad_PCA_results.json\n",
            "\n",
            "💾 New best model saved to best_checkpoint_40982l.pth (acc=0.7848, T=100, z=10)\n",
            "dim_hidden 500, latent_dim: 15\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.2796\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.2411\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.2403\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.1500\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.1026\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.1119\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.0931\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.0936\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.0832\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.0865\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "⏹️ Stopping early at epoch 1\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.7776\n",
            "ARI: 0.7114\n",
            "Saved results to clutad_PCA_results.json\n",
            "\n",
            "dim_hidden 500, latent_dim: 20\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.3118\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.2488\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.1771\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.1170\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.1125\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.1089\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.1008\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.0953\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.0930\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.0844\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "⏹️ Stopping early at epoch 22\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.6528\n",
            "ARI: 0.4179\n",
            "Saved results to clutad_PCA_results.json\n",
            "\n",
            "dim_hidden 1000, latent_dim: 5\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.2471\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.2151\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.1650\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.1474\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.0968\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.0983\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.0993\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.1043\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.0916\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.0777\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "Epoch 100/1000, Loss: 0.1557, Recon-Loss: 0.0726, Cluster-Loss: 0.8304\n",
            "Epoch 200/1000, Loss: 0.1409, Recon-Loss: 0.0839, Cluster-Loss: 0.5704\n",
            "Epoch 300/1000, Loss: 0.1382, Recon-Loss: 0.0891, Cluster-Loss: 0.4914\n",
            "Epoch 400/1000, Loss: 0.1233, Recon-Loss: 0.0772, Cluster-Loss: 0.4608\n",
            "Epoch 500/1000, Loss: 0.1133, Recon-Loss: 0.0689, Cluster-Loss: 0.4445\n",
            "Epoch 600/1000, Loss: 0.1147, Recon-Loss: 0.0716, Cluster-Loss: 0.4314\n",
            "Epoch 700/1000, Loss: 0.1012, Recon-Loss: 0.0583, Cluster-Loss: 0.4293\n",
            "Epoch 800/1000, Loss: 0.1299, Recon-Loss: 0.0872, Cluster-Loss: 0.4272\n",
            "Epoch 900/1000, Loss: 0.1227, Recon-Loss: 0.0810, Cluster-Loss: 0.4171\n",
            "Epoch 1000/1000, Loss: 0.1141, Recon-Loss: 0.0720, Cluster-Loss: 0.4205\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.8609\n",
            "ARI: 0.7813\n",
            "Saved results to clutad_PCA_results.json\n",
            "\n",
            "💾 New best model saved to best_checkpoint_40982l.pth (acc=0.8609, T=100, z=5)\n",
            "dim_hidden 1000, latent_dim: 10\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.2881\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.2287\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.1660\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.1043\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.1043\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.1216\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.0870\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.0846\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.0687\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.0769\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "Epoch 100/1000, Loss: 0.2773, Recon-Loss: 0.1890, Cluster-Loss: 0.8823\n",
            "Epoch 200/1000, Loss: 0.5050, Recon-Loss: 0.3842, Cluster-Loss: 1.2082\n",
            "Epoch 300/1000, Loss: 0.4529, Recon-Loss: 0.3006, Cluster-Loss: 1.5233\n",
            "Epoch 400/1000, Loss: 0.4687, Recon-Loss: 0.3491, Cluster-Loss: 1.1955\n",
            "Epoch 500/1000, Loss: 0.6407, Recon-Loss: 0.5172, Cluster-Loss: 1.2341\n",
            "⏹️ Stopping early at epoch 562\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.6064\n",
            "ARI: 0.3989\n",
            "Saved results to clutad_PCA_results.json\n",
            "\n",
            "dim_hidden 1000, latent_dim: 15\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.2405\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.2324\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.1583\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.1189\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.1078\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.1001\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.0864\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.0966\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.0840\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.0721\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "⏹️ Stopping early at epoch 42\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.6944\n",
            "ARI: 0.4993\n",
            "Saved results to clutad_PCA_results.json\n",
            "\n",
            "dim_hidden 1000, latent_dim: 20\n",
            "🔹 Starting pretraining...\n",
            "[Pretrain] Epoch 100/1000, Loss: 0.2588\n",
            "[Pretrain] Epoch 200/1000, Loss: 0.2174\n",
            "[Pretrain] Epoch 300/1000, Loss: 0.1371\n",
            "[Pretrain] Epoch 400/1000, Loss: 0.1170\n",
            "[Pretrain] Epoch 500/1000, Loss: 0.1555\n",
            "[Pretrain] Epoch 600/1000, Loss: 0.0886\n",
            "[Pretrain] Epoch 700/1000, Loss: 0.0820\n",
            "[Pretrain] Epoch 800/1000, Loss: 0.0951\n",
            "[Pretrain] Epoch 900/1000, Loss: 0.0912\n",
            "[Pretrain] Epoch 1000/1000, Loss: 0.0864\n",
            "🔹 Fitting initial GMM...\n",
            "🔹 Starting EM training...\n",
            "⏹️ Stopping early at epoch 1\n",
            "✅ Final clustering performance:\n",
            "Accuracy: 0.9144\n",
            "ARI: 0.7856\n",
            "Saved results to clutad_PCA_results.json\n",
            "\n",
            "💾 New best model saved to best_checkpoint_40982l.pth (acc=0.9144, T=100, z=20)\n",
            "🏁 Tuning finished.\n",
            "Best acc: 0.9144\n",
            "Best config: dim_hidden=1000, latent_dim=20\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, adjusted_rand_score\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# Define cluster alignment function\n",
        "def cluster_accuracy(y_true, y_pred):\n",
        "    contingency = confusion_matrix(y_true, y_pred)\n",
        "    row_ind, col_ind = linear_sum_assignment(-contingency)\n",
        "    mapping = dict(zip(col_ind, row_ind))\n",
        "    y_aligned = np.array([mapping[label] for label in y_pred])\n",
        "    acc = accuracy_score(y_true, y_aligned)\n",
        "    return acc, y_aligned\n",
        "\n",
        "\n",
        "dataset_index = '40982l'\n",
        "\n",
        "\n",
        "\n",
        "# Config\n",
        "DATA_PATH = 'data_processed.csv'\n",
        "CHECKPOINT_PATH_PRE = 'pretrain_checkpoint.pth'\n",
        "CHECKPOINT_PATH_FINAL = 'final_checkpoint.pth'\n",
        "LABEL_PATH = 'clusters.csv'\n",
        "METADATA_PATH = 'metadata.json'\n",
        "\n",
        "\n",
        "with open(METADATA_PATH, 'r') as f:\n",
        "  metadata = json.load(f)\n",
        "num_numeric = metadata['num_numerical_features']\n",
        "categories = metadata['num_classes_per_cat']\n",
        "n_clusters = metadata['num_clusters']\n",
        "\n",
        "T = 100 # this is a question !!!\n",
        "\n",
        "pretrain_steps = 1000 # same as in example\n",
        "em_epochs = 1000 # same as in example (it is 1000 altogether)\n",
        "batch_size = 256 # same as in example\n",
        "hidden_dims=[500, 500, 2000] # same as in example\n",
        "kl_weight=0.1 # same as in example\n",
        "lr = 1e-3 # same as in example\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "x_real = torch.tensor(df.values, dtype=torch.float32).to(device)\n",
        "dataset = TensorDataset(x_real)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "# Load ground truth\n",
        "y_true = pd.read_csv(LABEL_PATH).values.flatten()\n",
        "if y_true.dtype.kind in {'U', 'S', 'O'}:\n",
        "    unique_labels, y_true = np.unique(np.asarray(y_true).astype(str), return_inverse=True)\n",
        "N, D = x_real.shape\n",
        "\n",
        "BEST_PATH = f\"best_checkpoint_{dataset_index}.pth\"\n",
        "best_acc = -1.0\n",
        "best_meta = None\n",
        "dim_hidden = 1000\n",
        "latent_dim = 15\n",
        "#for kl_weight in range(2, 11):\n",
        " # kl_weight = kl_weight/10\n",
        "  #print(f\"kl_weight: {kl_weight}\")\n",
        "for dim_hidden in [500, 1000]:\n",
        "  for latent_dim in [5, 10, 15, 20]:\n",
        "    print(f\"dim_hidden {dim_hidden}, latent_dim: {latent_dim}\")\n",
        "\n",
        "    # Models\n",
        "    encoder = TabularEncoder(\n",
        "        input_dim=D,\n",
        "        hidden_dims=hidden_dims,\n",
        "        latent_dim=latent_dim\n",
        "    ).to(device)\n",
        "\n",
        "    denoiser = Denoiser(\n",
        "        dim_in=D,\n",
        "        latent_dim=latent_dim,\n",
        "        dim_hidden=dim_hidden,\n",
        "        num_numeric=num_numeric,\n",
        "        categories=categories\n",
        "    ).to(device)\n",
        "\n",
        "    # CluTaD wrapper\n",
        "    model = CluTaD(\n",
        "        encoder=encoder,\n",
        "        denoiser=denoiser,\n",
        "        T=T,\n",
        "        num_numeric=num_numeric,\n",
        "        categories=categories,\n",
        "        n_clusters=n_clusters,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(\n",
        "        list(encoder.parameters()) +\n",
        "        list(denoiser.parameters()) +\n",
        "        list(model.mlp.parameters()),\n",
        "        lr=lr, weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "    # 🔹 Pretraining\n",
        "    print(\"🔹 Starting pretraining...\")\n",
        "    model.pretrain(dataloader, optimizer, epochs=pretrain_steps, batch_size=batch_size, plot_freq=100)\n",
        "\n",
        "    # Save pretraining checkpoint\n",
        "    #os.makedirs(os.path.dirname(CHECKPOINT_PATH_PRE), exist_ok=True)\n",
        "    #torch.save({\n",
        "    #    'encoder': encoder.state_dict(),\n",
        "    #    'denoiser': denoiser.state_dict(),\n",
        "    #    'optimizer': optimizer.state_dict(),\n",
        "    #    'T': T,\n",
        "    #    'num_numeric': num_numeric,\n",
        "    #    'categories': categories\n",
        "    #}, CHECKPOINT_PATH_PRE)\n",
        "    #print(f\"✅ Pretraining checkpoint saved at {CHECKPOINT_PATH_PRE}\")\n",
        "\n",
        "    # 🔹 Fit initial GMM (E-step 0)\n",
        "    print(\"🔹 Fitting initial GMM...\")\n",
        "    model.fit_gmm(dataloader)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(\n",
        "        list(encoder.parameters()) +\n",
        "        list(denoiser.parameters()) +\n",
        "        list(model.mlp.parameters()),\n",
        "        lr=lr, weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "    # 🔹 EM training loop\n",
        "    print(\"🔹 Starting EM training...\")\n",
        "    for epoch in range(em_epochs):\n",
        "        avg_loss, avg_recon_loss, avg_cluster_loss, stop = model.train_elbo(\n",
        "            dataloader, optimizer, batch_size=batch_size, kl_weight=kl_weight, plot_freq=50\n",
        "        )\n",
        "        if (epoch+1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{em_epochs}, \"\n",
        "                  f\"Loss: {avg_loss:.4f}, Recon-Loss: {avg_recon_loss:.4f}, \"\n",
        "                  f\"Cluster-Loss: {avg_cluster_loss:.4f}\")\n",
        "\n",
        "        if stop:\n",
        "            print(f\"⏹️ Stopping early at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          # E-step\n",
        "            model.fit_gmm(dataloader)\n",
        "\n",
        "        #with torch.no_grad():\n",
        "          # mu, logvar = model.encoder(x_real)\n",
        "          #z_np = mu.cpu().numpy()\n",
        "          #y_pred = model.gmm.predict(z_np)\n",
        "\n",
        "          # Compute metrics\n",
        "          #accuracy, y_aligned = cluster_accuracy(y_true, y_pred)\n",
        "          #print(y_pred)\n",
        "          #print(accuracy)\n",
        "    # Save final model\n",
        "    #os.makedirs(os.path.dirname(CHECKPOINT_PATH_FINAL), exist_ok=True)\n",
        "    #torch.save({\n",
        "    #    'encoder': encoder.state_dict(),\n",
        "    #    'denoiser': denoiser.state_dict(),\n",
        "    #    'optimizer': optimizer.state_dict(),\n",
        "    #    'gmm': model.gmm\n",
        "    #}, CHECKPOINT_PATH_FINAL)\n",
        "    #print(f\"✅ Final checkpoint saved at {CHECKPOINT_PATH_FINAL}\")\n",
        "\n",
        "\n",
        "    # Encode all data and compute GMM assignments\n",
        "    with torch.no_grad():\n",
        "        mu, logvar = model.encoder(x_real)\n",
        "        z_np = mu.cpu().numpy()\n",
        "        y_pred = model.gmm.predict(z_np)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy, y_aligned = cluster_accuracy(y_true, y_pred)\n",
        "    ari = adjusted_rand_score(y_true, y_pred)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"✅ Final clustering performance:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"ARI: {ari:.4f}\")\n",
        "\n",
        "\n",
        "    # Example: after you finish one training run\n",
        "    results = {\n",
        "        \"accuracy\": float(accuracy),      # your computed accuracy\n",
        "        \"ari\": float(ari),                # your computed ARI\n",
        "        \"T\": int(T),                      # diffusion timesteps\n",
        "        \"dim_hidden\": int(dim_hidden),    # hidden dimension\n",
        "        \"latent_dim\": int(latent_dim),    # latent dimension\n",
        "        \"dataset_index\": str(dataset_index),      # e.g., \"mnist\", \"cifar10\", etc.\n",
        "        \"lambda\": float(kl_weight)\n",
        "    }\n",
        "\n",
        "    # Path to results file\n",
        "    results_file = Path(\"PCA/clutad_pca_results.json\")\n",
        "\n",
        "    # If file exists, load and append; else create new\n",
        "    if results_file.exists():\n",
        "        with open(results_file, \"r\") as f:\n",
        "            all_results = json.load(f)\n",
        "    else:\n",
        "        all_results = []\n",
        "\n",
        "    all_results.append(results)\n",
        "\n",
        "    # Save updated results\n",
        "    with open(results_file, \"w\") as f:\n",
        "        json.dump(all_results, f, indent=4)\n",
        "\n",
        "    print(f\"Saved results to {results_file}\\n\")\n",
        "\n",
        "    # --- NEW: update 'best' and save checkpoint if improved ---\n",
        "    if accuracy > best_acc:\n",
        "        best_acc = float(accuracy)\n",
        "        best_meta = {\n",
        "            \"T\": int(T),\n",
        "            \"latent_dim\": int(latent_dim),\n",
        "            \"dim_hidden\": int(dim_hidden),\n",
        "            \"dataset_index\": str(dataset_index),\n",
        "            \"accuracy\": float(accuracy),\n",
        "            \"ari\": float(ari)\n",
        "        }\n",
        "\n",
        "        # save immediately so you don't lose it if the script stops\n",
        "        torch.save({\n",
        "            \"encoder\": encoder.state_dict(),\n",
        "            \"denoiser\": denoiser.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),  # optional but handy\n",
        "            \"gmm\": model.gmm,                     # sklearn object; torch.save pickles it\n",
        "            \"config\": {\n",
        "                \"T\": T,\n",
        "                \"num_numeric\": num_numeric,\n",
        "                \"categories\": categories,\n",
        "                \"n_clusters\": n_clusters,\n",
        "                \"hidden_dims\": hidden_dims,\n",
        "                \"dim_hidden\": dim_hidden,\n",
        "                \"latent_dim\": latent_dim,\n",
        "                \"kl_weight\": kl_weight,\n",
        "                \"lr\": lr,\n",
        "                \"batch_size\": batch_size,\n",
        "                \"em_epochs\": em_epochs,\n",
        "                \"pretrain_steps\": pretrain_steps,\n",
        "            },\n",
        "            \"metrics\": {\n",
        "                \"accuracy\": float(accuracy),\n",
        "                \"ari\": float(ari)\n",
        "            }\n",
        "        }, BEST_PATH)\n",
        "\n",
        "        print(f\"💾 New best model saved to {BEST_PATH} (acc={accuracy:.4f}, T={T}, z={latent_dim})\")\n",
        "    # ----------------------------------------------------------\n",
        "\n",
        "# --- after both loops finish ---\n",
        "print(\"🏁 Tuning finished.\")\n",
        "print(f\"Best acc: {best_acc:.4f}\")\n",
        "if best_meta is not None:\n",
        "    print(f\"Best config: dim_hidden={best_meta['dim_hidden']}, latent_dim={best_meta['latent_dim']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
